Claro! Vamos realizar a tarefa de treinar um classificador SVM utilizando **embeddings de palavras**. Neste caso, em vez de usar a representação Bag-of-Words, utilizaremos embeddings para capturar relações semânticas entre as palavras, o que pode melhorar o desempenho do modelo.

### **Passos a Seguir**

1. **Carregar os dados pré-processados** do arquivo CSV resultante do pré-processamento.
2. **Obter embeddings para as palavras** usando um modelo pré-treinado.
3. **Representar cada avaliação como um vetor** obtido a partir dos embeddings das palavras.
4. **Dividir os dados** em conjuntos de treinamento e teste.
5. **Treinar o classificador SVM** no conjunto de treinamento.
6. **Avaliar o modelo** no conjunto de teste usando métricas como acurácia e F1-score.
7. **Exibir os resultados**.

### **Escolhendo Embeddings**

Para obter os embeddings das palavras, podemos utilizar modelos pré-treinados, como:

- **Word2Vec**: Disponível no `gensim`.
- **FastText**: Também disponível no `gensim` ou no `fasttext`.
- **GloVe**: Embeddings pré-treinados disponibilizados pela Stanford.

Para o português, podemos usar modelos de embeddings treinados na língua portuguesa. Vamos utilizar o **Word2Vec** treinado em português, disponibilizado pela NILC (Núcleo Interinstitucional de Linguística Computacional):

- **Link para download**: [Modelos Word2Vec NILC](http://nilc.icmc.usp.br/embeddings)

### **Código Completo**

```python
# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
import gensim
from gensim.models import KeyedVectors
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import nltk

# Certifique-se de que o NLTK está atualizado
nltk.download('punkt')

# 1. Carregar os dados pré-processados
df = pd.read_csv('avaliacoes_preprocessadas.csv')

# Exibir as primeiras linhas do DataFrame
print("Dados pré-processados:")
print(df.head())

# Verificar se há valores nulos
print("\nVerificando valores nulos:")
print(df.isnull().sum())

# Remover linhas com valores nulos (se houver)
df.dropna(inplace=True)

# 2. Carregar o modelo de embeddings pré-treinado

# Especificar o caminho para o arquivo de embeddings (baixe o modelo e indique o caminho correto)
# Por exemplo, 'embeddings/skip_s50.txt' para o modelo Skip-gram com vetores de 50 dimensões
embedding_path = 'skip_s50.txt'

# Carregar o modelo de embeddings
print("\nCarregando o modelo de embeddings...")
word_vectors = KeyedVectors.load_word2vec_format(embedding_path)

# Verificar o tamanho dos vetores de embeddings
embedding_dim = word_vectors.vector_size
print(f"Dimensão dos embeddings: {embedding_dim}")

# 3. Representar cada avaliação como um vetor de embeddings

def get_embedding_vector(text):
    tokens = nltk.word_tokenize(text)
    # Lista para armazenar os embeddings das palavras presentes no vocabulário
    vectors = []
    for token in tokens:
        if token in word_vectors:
            vectors.append(word_vectors[token])
    if vectors:
        # Média dos vetores das palavras para representar o texto
        return np.mean(vectors, axis=0)
    else:
        # Se nenhuma palavra estiver no vocabulário, retorna um vetor de zeros
        return np.zeros(embedding_dim)

# Aplicar a função aos textos
print("\nGerando embeddings para os textos...")
df['embedding'] = df['clean_review'].apply(get_embedding_vector)

# Converter a coluna de embeddings em um array numpy
X = np.vstack(df['embedding'].values)
y = df['sentiment']

# 4. Dividir os dados em conjuntos de treinamento e teste

# Dividir em treinamento e teste (80% treinamento, 20% teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 5. Treinar o classificador SVM

# Inicializar o classificador SVM (usando kernel linear para velocidade)
svm_classifier = SVC(kernel='linear', random_state=42)

# Treinar o modelo
print("\nTreinando o modelo SVM...")
svm_classifier.fit(X_train, y_train)

# 6. Avaliar o modelo no conjunto de teste

# Prever os sentimentos no conjunto de teste
y_pred = svm_classifier.predict(X_test)

# Calcular a acurácia
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAcurácia no conjunto de teste: {accuracy:.4f}")

# Exibir o relatório de classificação
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred, digits=4))

# Matriz de Confusão
conf_matrix = confusion_matrix(y_test, y_pred, labels=['positivo', 'neutro', 'negativo'])

# Visualizar a matriz de confusão
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Positivo', 'Neutro', 'Negativo'], yticklabels=['Positivo', 'Neutro', 'Negativo'])
plt.title('Matriz de Confusão')
plt.xlabel('Predito')
plt.ylabel('Verdadeiro')
plt.show()
```

### **Explicação do Código**

#### **1. Carregar os dados pré-processados**

- **Leitura do arquivo CSV**: Carregamos o DataFrame a partir do arquivo `avaliacoes_preprocessadas.csv`.
- **Verificação de valores nulos**: Checamos se há valores nulos e os removemos se necessário.

#### **2. Carregar o modelo de embeddings pré-treinado**

- **Download dos Embeddings**:
  - Baixe o modelo de embeddings em português, como o **skip_s50.txt**, disponível no [NILC](http://nilc.icmc.usp.br/embeddings).
- **Carregar o Modelo**:
  - Usamos `KeyedVectors.load_word2vec_format` para carregar o modelo no formato Word2Vec.
- **Obter a Dimensão dos Embeddings**:
  - `word_vectors.vector_size` retorna a dimensão dos vetores (por exemplo, 50).

#### **3. Representar cada avaliação como um vetor de embeddings**

- **Função `get_embedding_vector`**:
  - Tokeniza o texto e obtém os embeddings de cada palavra.
  - Calcula a média dos vetores das palavras presentes no vocabulário para representar o texto.
  - Se nenhuma palavra estiver no vocabulário, retorna um vetor de zeros.
- **Aplicar a Função aos Textos**:
  - Geramos um vetor de embeddings para cada avaliação e armazenamos na coluna `embedding`.
- **Preparar os Dados**:
  - Convertemos a coluna de embeddings em um array numpy (`X`).
  - `y` contém os rótulos de sentimento.

#### **4. Dividir os dados em conjuntos de treinamento e teste**

- **Divisão dos Dados**:
  - Usamos `train_test_split` para dividir `X` e `y`.

#### **5. Treinar o classificador SVM**

- **Inicialização do SVM**:
  - Usamos o kernel linear.
- **Treinamento do Modelo**:
  - Treinamos o modelo com `X_train` e `y_train`.

#### **6. Avaliar o modelo no conjunto de teste**

- **Predição no Conjunto de Teste**:
  - Usamos o modelo treinado para prever os sentimentos em `X_test`.
- **Cálculo da Acurácia**:
  - Comparamos as predições com os valores verdadeiros (`y_test`).
- **Relatório de Classificação**:
  - Obtemos métricas como precisão, recall e F1-score.
- **Matriz de Confusão**:
  - Visualizamos a performance do modelo em cada classe.

### **Preparação do Ambiente**

- **Download dos Embeddings**:
  - Acesse [NILC Embeddings](http://nilc.icmc.usp.br/embeddings) e baixe o arquivo **skip_s50.zip**.
  - Extraia o arquivo e coloque o arquivo `skip_s50.txt` no diretório do seu projeto.
- **Instalar o Gensim**:
  - Certifique-se de que o `gensim` está instalado:
    ```bash
    pip install gensim
    ```

### **Observações Importantes**

- **Compatibilidade de Palavras**:
  - Nem todas as palavras das avaliações podem estar presentes no vocabulário dos embeddings.
  - A função `get_embedding_vector` lida com isso ao ignorar palavras ausentes.
- **Dimensão dos Embeddings**:
  - Modelos com dimensões maiores podem capturar mais nuances semânticas, mas podem aumentar o tempo de treinamento.
- **Velocidade**:
  - O processo de gerar embeddings pode ser demorado para conjuntos de dados grandes.

### **Possíveis Melhorias**

- **Utilizar Embeddings Contextuais**:
  - Modelos como **BERT** ou **FastText** podem oferecer embeddings mais ricos.
  - O FastText lida melhor com palavras fora do vocabulário ao considerar subpalavras.
- **Ajuste de Hiperparâmetros**:
  - Ajustar os parâmetros do SVM, como o `C`, pode melhorar o desempenho.
- **Dimensionalidade**:
  - Se necessário, aplicar **redução de dimensionalidade** (por exemplo, PCA) antes de treinar o modelo.

### **Próximos Passos**

- **Comparar com o Modelo Anterior**:
  - Compare o desempenho deste modelo com o SVM + BoW.
- **Treinar o Modelo BERT**:
  - Implementar a próxima tarefa usando o BERT para classificação.
- **Análise de Erros**:
  - Identificar onde o modelo está errando para possíveis melhorias.

### **Conclusão**

O uso de embeddings de palavras permite capturar relações semânticas entre as palavras, potencialmente melhorando o desempenho do classificador em relação ao uso de Bag-of-Words. É importante analisar os resultados e interpretar as métricas para avaliar se essa abordagem oferece benefícios significativos para o seu conjunto de dados.

Se tiver alguma dúvida ou precisar de assistência adicional, estou à disposição para ajudar!