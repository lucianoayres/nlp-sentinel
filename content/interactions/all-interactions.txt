// File: content/interactions/1-pre-processamento-dados.txt

Claro! Vou ajudá-lo a gerar o código para o pré-processamento dos dados de avaliações de produtos extraídos de um site de e-commerce fictício.

### Assumindo o Conjunto de Dados

Vamos supor que temos um arquivo CSV chamado `avaliacoes.csv` com as seguintes colunas:

- **`review_text`**: Texto da avaliação do usuário.
- **`rating`**: Nota dada pelo usuário (por exemplo, de 1 a 5).

### Objetivos do Pré-processamento

1. **Carregar os dados** do arquivo CSV.
2. **Limpar e pré-processar o texto** das avaliações:
   - Remover pontuações e caracteres especiais.
   - Converter o texto para letras minúsculas.
   - Remover stopwords (palavras comuns que não agregam significado significativo).
   - Realizar tokenização.
   - (Opcional) Aplicar lematização ou stemming.
3. **Converter as notas em rótulos de sentimento**:
   - Notas 4 e 5: **Positivo**.
   - Nota 3: **Neutro**.
   - Notas 1 e 2: **Negativo**.

### Código para o Pré-processamento

```python
# Importando as bibliotecas necessárias
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Baixar recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# 1. Carregar os dados
df = pd.read_csv('avaliacoes.csv')

# Exibir as primeiras linhas do DataFrame
print("Dados originais:")
print(df.head())

# 2. Limpar e pré-processar o texto

# Definir stopwords em português
stop_words = set(stopwords.words('portuguese'))

def preprocess_text(text):
    # Remover números
    text = re.sub(r'\d+', '', text)
    # Remover pontuação e caracteres especiais
    text = re.sub(r'[^\w\s]', '', text)
    # Converter para minúsculas
    text = text.lower()
    # Tokenizar o texto
    tokens = word_tokenize(text)
    # Remover stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # (Opcional) Aplicar lematização ou stemming
    # Retornar o texto pré-processado
    return ' '.join(tokens)

# Aplicar o pré-processamento ao texto das avaliações
df['clean_review'] = df['review_text'].apply(preprocess_text)

# 3. Converter as notas em rótulos de sentimento

def get_sentiment_label(rating):
    if rating >= 4:
        return 'positivo'
    elif rating == 3:
        return 'neutro'
    else:
        return 'negativo'

df['sentiment'] = df['rating'].apply(get_sentiment_label)

# Exibir as primeiras linhas do DataFrame após o pré-processamento
print("\nDados após o pré-processamento:")
print(df.head())

# Salvar o DataFrame pré-processado em um novo arquivo CSV
df.to_csv('avaliacoes_preprocessadas.csv', index=False)
```

### Explicação do Código

1. **Importação das bibliotecas**: Importamos as bibliotecas necessárias para manipulação de dados (`pandas`, `numpy`), expressões regulares (`re`) e processamento de linguagem natural (`nltk`).

2. **Download de recursos do NLTK**: Baixamos os recursos necessários do NLTK, como tokenizadores e listas de stopwords.

3. **Carregamento dos dados**: Utilizamos `pd.read_csv` para ler o arquivo `avaliacoes.csv` e carregá-lo em um DataFrame do pandas.

4. **Definição das stopwords**: Obtemos a lista de stopwords em português para serem usadas na remoção de palavras irrelevantes.

5. **Função `preprocess_text`**:
   - **Remoção de números**: Usamos `re.sub(r'\d+', '', text)` para remover dígitos.
   - **Remoção de pontuação e caracteres especiais**: Usamos `re.sub(r'[^\w\s]', '', text)` para manter apenas letras e espaços.
   - **Conversão para minúsculas**: Facilita a padronização do texto.
   - **Tokenização**: Quebramos o texto em uma lista de palavras individuais.
   - **Remoção de stopwords**: Removemos palavras comuns que não adicionam significado.
   - **(Opcional) Lematização ou stemming**: Pode ser adicionado para reduzir as palavras às suas raízes.

6. **Aplicação do pré-processamento**: Aplicamos a função `preprocess_text` à coluna `review_text` e armazenamos o resultado na nova coluna `clean_review`.

7. **Conversão das notas em rótulos de sentimento**:
   - Criamos a função `get_sentiment_label` que mapeia as notas numéricas para os rótulos de sentimento correspondentes.
   - Aplicamos essa função à coluna `rating` para criar a nova coluna `sentiment`.

8. **Visualização dos dados pré-processados**: Exibimos as primeiras linhas do DataFrame após o pré-processamento para verificar o resultado.

9. **Salvamento dos dados pré-processados**: Salvamos o DataFrame resultante em um novo arquivo CSV chamado `avaliacoes_preprocessadas.csv`.

### Observações

- **Lematização ou Stemming**: Se desejar aplicar lematização ou stemming para reduzir as palavras às suas formas básicas, você pode utilizar as seguintes bibliotecas:
  - **Stemming**: `nltk.stem.RSLPStemmer` para português.
  - **Lematização**: Bibliotecas como `spacy` com suporte para português.

- **Exemplo de Aplicação de Stemming**:

  ```python
  from nltk.stem import RSLPStemmer
  stemmer = RSLPStemmer()

  def preprocess_text(text):
      # ... código anterior ...
      # Aplicar stemming
      tokens = [stemmer.stem(word) for word in tokens]
      # Retornar o texto pré-processado
      return ' '.join(tokens)
  ```

- **Stopwords Personalizadas**: Você pode adicionar ou remover palavras da lista de stopwords conforme necessário para o seu projeto.

- **Validação dos Dados**: Sempre é uma boa prática verificar se há valores nulos ou duplicados e tratá-los adequadamente.

### Próximos Passos

Com os dados pré-processados, você pode prosseguir para:

- **Dividir o conjunto de dados** em treinamento e teste.
- **Extrair features** dos textos pré-processados para serem usadas pelos classificadores:
  - **SVM + Bag-of-Words**: Usar `CountVectorizer`.
  - **SVM + Embeddings**: Usar embeddings pré-treinados como Word2Vec ou FastText.
  - **BERT**: Preparar os dados para serem alimentados em um modelo BERT.

Se tiver alguma dúvida ou precisar de assistência nas próximas etapas, estou à disposição para ajudar!
// File: content/interactions/2-analise-de-dados

Claro! Vamos realizar análises descritivas e inferenciais dos dados pré-processados. As análises nos ajudarão a entender melhor a distribuição dos sentimentos, das notas e outras características importantes das avaliações.

### **Análises Descritivas**

#### 1. **Distribuição das Notas**

Vamos analisar como as notas (ratings) estão distribuídas no conjunto de dados.

```python
# Importar bibliotecas adicionais para visualização
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo dos gráficos
sns.set(style="whitegrid")

# Contagem de avaliações por nota
rating_counts = df['rating'].value_counts().sort_index()

print("Contagem de avaliações por nota:")
print(rating_counts)

# Plotar histograma das notas
plt.figure(figsize=(8,6))
sns.countplot(x='rating', data=df, order=sorted(df['rating'].unique()))
plt.title('Distribuição das Notas')
plt.xlabel('Nota')
plt.ylabel('Contagem')
plt.show()
```

**Explicação:**

- **Contagem de avaliações por nota**: Calculamos quantas avaliações existem para cada nota.
- **Visualização**: Usamos um gráfico de barras para visualizar a distribuição das notas.

#### 2. **Distribuição dos Sentimentos**

Agora, vamos verificar como os sentimentos estão distribuídos.

```python
# Contagem de avaliações por sentimento
sentiment_counts = df['sentiment'].value_counts()

print("\nContagem de avaliações por sentimento:")
print(sentiment_counts)

# Plotar gráfico de barras dos sentimentos
plt.figure(figsize=(8,6))
sns.countplot(x='sentiment', data=df, order=['positivo', 'neutro', 'negativo'])
plt.title('Distribuição dos Sentimentos')
plt.xlabel('Sentimento')
plt.ylabel('Contagem')
plt.show()
```

**Explicação:**

- **Contagem de avaliações por sentimento**: Vemos quantas avaliações estão classificadas como positivas, neutras ou negativas.
- **Visualização**: O gráfico de barras nos ajuda a comparar visualmente as frequências de cada sentimento.

#### 3. **Análise do Comprimento das Avaliações**

Vamos calcular o comprimento (número de palavras) de cada avaliação e analisar a distribuição por sentimento.

```python
# Calcular o número de palavras em cada avaliação pré-processada
df['review_length'] = df['clean_review'].apply(lambda x: len(x.split()))

# Exibir estatísticas descritivas do comprimento das avaliações
print("\nEstatísticas descritivas do comprimento das avaliações:")
print(df['review_length'].describe())

# Boxplot do comprimento das avaliações por sentimento
plt.figure(figsize=(8,6))
sns.boxplot(x='sentiment', y='review_length', data=df, order=['positivo', 'neutro', 'negativo'])
plt.title('Comprimento das Avaliações por Sentimento')
plt.xlabel('Sentimento')
plt.ylabel('Número de Palavras')
plt.show()
```

**Explicação:**

- **Cálculo do comprimento**: Contamos o número de palavras em cada avaliação.
- **Estatísticas descritivas**: Obtemos medidas como média, mediana, mínimo e máximo.
- **Visualização**: O boxplot mostra a distribuição do comprimento das avaliações para cada sentimento.

#### 4. **Palavras Mais Frequentes por Sentimento**

Vamos identificar as palavras mais frequentes em cada classe de sentimento.

```python
from collections import Counter

def get_top_n_words(corpus, n=None):
    tokens = [word for text in corpus for word in text.split()]
    return Counter(tokens).most_common(n)

# Palavras mais frequentes em avaliações positivas
positive_reviews = df[df['sentiment'] == 'positivo']['clean_review']
top_positive = get_top_n_words(positive_reviews, n=10)
print("\nPalavras mais frequentes em avaliações positivas:")
print(top_positive)

# Palavras mais frequentes em avaliações negativas
negative_reviews = df[df['sentiment'] == 'negativo']['clean_review']
top_negative = get_top_n_words(negative_reviews, n=10)
print("\nPalavras mais frequentes em avaliações negativas:")
print(top_negative)

# Palavras mais frequentes em avaliações neutras
neutral_reviews = df[df['sentiment'] == 'neutro']['clean_review']
top_neutral = get_top_n_words(neutral_reviews, n=10)
print("\nPalavras mais frequentes em avaliações neutras:")
print(top_neutral)
```

**Explicação:**

- **Contagem de palavras**: Contamos a frequência de cada palavra nas avaliações de cada sentimento.
- **Interpretação**: As palavras mais frequentes podem indicar temas ou aspectos recorrentes em cada sentimento.

#### 5. **Exemplos de Avaliações por Sentimento**

Visualizar exemplos reais de avaliações ajuda a compreender melhor o conteúdo.

```python
# Exemplo de avaliação positiva
print("\nExemplo de avaliação positiva:")
print(df[df['sentiment'] == 'positivo']['review_text'].iloc[0])

# Exemplo de avaliação negativa
print("\nExemplo de avaliação negativa:")
print(df[df['sentiment'] == 'negativo']['review_text'].iloc[0])

# Exemplo de avaliação neutra
print("\nExemplo de avaliação neutra:")
print(df[df['sentiment'] == 'neutro']['review_text'].iloc[0])
```

**Explicação:**

- **Exemplos práticos**: Mostramos um exemplo de avaliação para cada sentimento para entender o tipo de feedback dos usuários.

### **Análises Inferenciais**

Vamos realizar uma análise inferencial para verificar se há diferenças significativas no comprimento das avaliações entre os diferentes sentimentos.

#### 1. **Teste ANOVA**

Como temos três grupos (positivo, neutro, negativo), podemos utilizar a ANOVA para testar se há diferenças significativas no comprimento médio das avaliações entre os grupos.

```python
from scipy.stats import f_oneway

# Separar o comprimento das avaliações por sentimento
positive_lengths = df[df['sentiment'] == 'positivo']['review_length']
neutral_lengths = df[df['sentiment'] == 'neutro']['review_length']
negative_lengths = df[df['sentiment'] == 'negativo']['review_length']

# Realizar o teste ANOVA
f_statistic, p_value = f_oneway(positive_lengths, neutral_lengths, negative_lengths)

print("\nResultado do teste ANOVA:")
print(f"Estatística F: {f_statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

**Interpretação:**

- **Hipótese Nula (H₀)**: As médias dos comprimentos das avaliações são iguais entre os grupos.
- **Hipótese Alternativa (H₁)**: Pelo menos uma das médias dos grupos é diferente.
- **Análise do Valor-p**: Se o valor-p for menor que 0,05, rejeitamos H₀, indicando diferenças significativas.

#### 2. **Teste t de Student (para comparação entre dois grupos)**

Se quisermos comparar apenas dois grupos (por exemplo, positivo e negativo), podemos usar o teste t de Student.

```python
from scipy.stats import ttest_ind

# Comparar avaliações positivas e negativas
t_statistic, p_value = ttest_ind(positive_lengths, negative_lengths, equal_var=False)

print("\nResultado do teste t de Student (positivo vs negativo):")
print(f"Estatística t: {t_statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

**Interpretação:**

- **Hipótese Nula (H₀)**: As médias dos comprimentos das avaliações positivas e negativas são iguais.
- **Hipótese Alternativa (H₁)**: As médias dos comprimentos das avaliações são diferentes.
- **Análise do Valor-p**: Um valor-p menor que 0,05 indica diferença significativa.

#### 3. **Análise de Correlação**

Podemos analisar se existe correlação entre a nota (rating) e o comprimento da avaliação.

```python
# Calcular correlação de Pearson
correlation = df['rating'].corr(df['review_length'])

print("\nCorrelação entre nota e comprimento da avaliação:")
print(f"Coeficiente de correlação de Pearson: {correlation:.4f}")
```

**Interpretação:**

- **Coeficiente de Correlação**:
  - Próximo de 1: forte correlação positiva.
  - Próximo de -1: forte correlação negativa.
  - Próximo de 0: pouca ou nenhuma correlação linear.

### **Conclusões**

- **Distribuição das Notas e Sentimentos**: A maioria das avaliações é positiva, indicando satisfação dos clientes, ou há um equilíbrio entre os sentimentos?
- **Comprimento das Avaliações**: Avaliações negativas tendem a ser mais detalhadas? Isso pode indicar que clientes insatisfeitos fornecem mais feedback.
- **Palavras Mais Frequentes**: As palavras mais usadas refletem aspectos específicos do produto ou serviço?
- **Diferenças Significativas**: Se os testes estatísticos indicam diferenças significativas, podemos afirmar que o sentimento influencia o comportamento de escrita dos usuários.
- **Correlação**: Uma correlação significativa entre nota e comprimento pode sugerir que avaliações com certas notas tendem a ser mais longas ou curtas.

### **Próximos Passos**

- **Visualizações Adicionais**: Criar nuvens de palavras (word clouds) para visualizar as palavras mais frequentes.
- **Análise de Sentimento Detalhada**: Explorar quais aspectos são mais mencionados em cada sentimento.
- **Preparação para Modelagem**: Utilizar os insights obtidos para orientar o treinamento dos classificadores.

---

**Observação**: Como estamos trabalhando com dados fictícios, os resultados são ilustrativos. Em um contexto real, essas análises forneceriam insights valiosos sobre o comportamento dos clientes e poderiam influenciar estratégias de negócio.

Se precisar de ajuda adicional ou tiver dúvidas sobre qualquer parte do código ou interpretação dos resultados, estou à disposição!
// File: content/interactions/3_1-treinamento-SVM+BoW.txt

Claro! Vamos gerar o código para treinar um classificador SVM utilizando a representação **Bag-of-Words (BoW)**. Esse modelo usará a frequência de palavras nas avaliações pré-processadas para prever o sentimento associado.

### **Passos a Seguir**

1. **Carregar os dados pré-processados** do arquivo CSV resultante do pré-processamento.
2. **Dividir os dados** em conjuntos de treinamento e teste.
3. **Converter o texto em representações BoW** usando `CountVectorizer`.
4. **Treinar o classificador SVM** no conjunto de treinamento.
5. **Avaliar o modelo** no conjunto de teste usando métricas como acurácia e F1-score.
6. **Exibir os resultados**.

### **Código Completo**

```python
# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Carregar os dados pré-processados
df = pd.read_csv('avaliacoes_preprocessadas.csv')

# Exibir as primeiras linhas do DataFrame
print("Dados pré-processados:")
print(df.head())

# Verificar se há valores nulos
print("\nVerificando valores nulos:")
print(df.isnull().sum())

# Remover linhas com valores nulos (se houver)
df.dropna(inplace=True)

# 2. Dividir os dados em conjuntos de treinamento e teste

# Separar as features (texto) e o target (sentimento)
X = df['clean_review']
y = df['sentiment']

# Dividir em treinamento e teste (80% treinamento, 20% teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 3. Converter o texto em representações BoW

# Inicializar o CountVectorizer
vectorizer = CountVectorizer()

# Ajustar e transformar os dados de treinamento
X_train_bow = vectorizer.fit_transform(X_train)

# Transformar os dados de teste
X_test_bow = vectorizer.transform(X_test)

# 4. Treinar o classificador SVM

# Inicializar o classificador SVM (usando kernel linear para velocidade)
svm_classifier = SVC(kernel='linear', random_state=42)

# Treinar o modelo
svm_classifier.fit(X_train_bow, y_train)

# 5. Avaliar o modelo no conjunto de teste

# Prever os sentimentos no conjunto de teste
y_pred = svm_classifier.predict(X_test_bow)

# Calcular a acurácia
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAcurácia no conjunto de teste: {accuracy:.4f}")

# Exibir o relatório de classificação
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred, digits=4))

# Matriz de Confusão
conf_matrix = confusion_matrix(y_test, y_pred, labels=['positivo', 'neutro', 'negativo'])

# Visualizar a matriz de confusão
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Positivo', 'Neutro', 'Negativo'], yticklabels=['Positivo', 'Neutro', 'Negativo'])
plt.title('Matriz de Confusão')
plt.xlabel('Predito')
plt.ylabel('Verdadeiro')
plt.show()
```

### **Explicação do Código**

#### **1. Carregar os dados pré-processados**

- **Leitura do arquivo CSV**: Carregamos o DataFrame a partir do arquivo `avaliacoes_preprocessadas.csv`.
- **Verificação de valores nulos**: Checamos se há valores nulos e os removemos se necessário.

#### **2. Dividir os dados em conjuntos de treinamento e teste**

- **Separação das features e target**:
  - `X`: Contém o texto pré-processado (`clean_review`).
  - `y`: Contém os rótulos de sentimento (`sentiment`).
- **Divisão dos dados**:
  - Usamos `train_test_split` para dividir os dados em treinamento e teste.
  - O parâmetro `stratify=y` garante que a proporção de classes seja mantida em ambos os conjuntos.
  - `random_state` é definido para reprodutibilidade.

#### **3. Converter o texto em representações BoW**

- **Inicialização do `CountVectorizer`**:
  - Converte o texto em uma matriz de frequências de palavras.
- **Ajuste e transformação dos dados de treinamento**:
  - `fit_transform` aprende o vocabulário e transforma o texto em vetores.
- **Transformação dos dados de teste**:
  - Usamos `transform` para converter o texto de teste com o vocabulário aprendido no conjunto de treinamento.

#### **4. Treinar o classificador SVM**

- **Inicialização do SVM**:
  - Usamos o kernel linear para simplificar o modelo e acelerar o treinamento.
- **Treinamento do modelo**:
  - O método `fit` treina o modelo nos dados de treinamento.

#### **5. Avaliar o modelo no conjunto de teste**

- **Predição no conjunto de teste**:
  - Usamos o modelo treinado para prever os sentimentos das avaliações de teste.
- **Cálculo da acurácia**:
  - Comparamos as predições com os valores verdadeiros.
- **Relatório de classificação**:
  - Usamos `classification_report` para obter métricas como precisão, recall e F1-score para cada classe.
- **Matriz de Confusão**:
  - Criamos uma matriz para visualizar o desempenho do modelo em cada classe.
  - Usamos `seaborn` para plotar a matriz de confusão.

### **Resultados Esperados**

- **Acurácia**: A proporção de predições corretas no conjunto de teste.
- **Relatório de Classificação**:
  - **Precisão (Precision)**: Proporção de predições positivas corretas.
  - **Recall**: Proporção de verdadeiros positivos identificados corretamente.
  - **F1-score**: Média harmônica entre precisão e recall.
- **Matriz de Confusão**:
  - Fornece uma visão detalhada de como o modelo está performando em cada classe.
  - Nos permite identificar onde o modelo está cometendo mais erros.

### **Interpretação dos Resultados**

- **Desempenho Geral**: Avalie se a acurácia e o F1-score são satisfatórios para o problema.
- **Classes Desbalanceadas**: Se uma classe tiver muito mais exemplos, pode afetar o desempenho. Considere técnicas de balanceamento se necessário.
- **Erros Comuns**:
  - Verifique na matriz de confusão se o modelo está confundindo sentimentos específicos.
  - Isso pode indicar a necessidade de ajustar o modelo ou melhorar o pré-processamento.

### **Possíveis Melhorias**

- **Ajuste de Hiperparâmetros**:
  - Use `GridSearchCV` ou `RandomizedSearchCV` para encontrar os melhores parâmetros para o SVM.
- **Remover Palavras Infrequentes**:
  - Ajuste os parâmetros do `CountVectorizer` para ignorar palavras muito raras ou muito comuns (`min_df`, `max_df`).
- **N-Gramas**:
  - Considere usar n-gramas (sequências de palavras) para capturar contexto adicional.
    ```python
    vectorizer = CountVectorizer(ngram_range=(1,2))
    ```
- **Normalização dos Dados**:
  - Utilize técnicas de normalização ou padronização nos dados vetorizados.
- **Feature Selection**:
  - Selecionar as features mais relevantes pode melhorar o desempenho.

### **Próximos Passos**

- **Comparar com Outros Modelos**:
  - Após treinar o SVM + BoW, treine também os outros modelos especificados (SVM + Embeddings, BERT).
  - Compare os resultados para determinar qual abordagem é mais eficaz.
- **Análise dos Erros**:
  - Examine exemplos específicos onde o modelo errou para entender possíveis causas.
- **Documentação**:
  - Registre os resultados, interpretações e insights para incluir na apresentação final.

### **Observações Finais**

- **Reprodutibilidade**:
  - Mantenha o `random_state` definido para garantir que os resultados possam ser reproduzidos.
- **Dados Fictícios**:
  - Como estamos trabalhando com dados fictícios, os resultados podem variar. Em um ambiente real, certifique-se de ter um conjunto de dados representativo.

Se tiver alguma dúvida ou precisar de assistência adicional, estou aqui para ajudar!
// File: content/interactions/3_2-treinamento-SVM+Embeddings.txt

Claro! Vamos realizar a tarefa de treinar um classificador SVM utilizando **embeddings de palavras**. Neste caso, em vez de usar a representação Bag-of-Words, utilizaremos embeddings para capturar relações semânticas entre as palavras, o que pode melhorar o desempenho do modelo.

### **Passos a Seguir**

1. **Carregar os dados pré-processados** do arquivo CSV resultante do pré-processamento.
2. **Obter embeddings para as palavras** usando um modelo pré-treinado.
3. **Representar cada avaliação como um vetor** obtido a partir dos embeddings das palavras.
4. **Dividir os dados** em conjuntos de treinamento e teste.
5. **Treinar o classificador SVM** no conjunto de treinamento.
6. **Avaliar o modelo** no conjunto de teste usando métricas como acurácia e F1-score.
7. **Exibir os resultados**.

### **Escolhendo Embeddings**

Para obter os embeddings das palavras, podemos utilizar modelos pré-treinados, como:

- **Word2Vec**: Disponível no `gensim`.
- **FastText**: Também disponível no `gensim` ou no `fasttext`.
- **GloVe**: Embeddings pré-treinados disponibilizados pela Stanford.

Para o português, podemos usar modelos de embeddings treinados na língua portuguesa. Vamos utilizar o **Word2Vec** treinado em português, disponibilizado pela NILC (Núcleo Interinstitucional de Linguística Computacional):

- **Link para download**: [Modelos Word2Vec NILC](http://nilc.icmc.usp.br/embeddings)

### **Código Completo**

```python
# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
import gensim
from gensim.models import KeyedVectors
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import nltk

# Certifique-se de que o NLTK está atualizado
nltk.download('punkt')

# 1. Carregar os dados pré-processados
df = pd.read_csv('avaliacoes_preprocessadas.csv')

# Exibir as primeiras linhas do DataFrame
print("Dados pré-processados:")
print(df.head())

# Verificar se há valores nulos
print("\nVerificando valores nulos:")
print(df.isnull().sum())

# Remover linhas com valores nulos (se houver)
df.dropna(inplace=True)

# 2. Carregar o modelo de embeddings pré-treinado

# Especificar o caminho para o arquivo de embeddings (baixe o modelo e indique o caminho correto)
# Por exemplo, 'embeddings/skip_s50.txt' para o modelo Skip-gram com vetores de 50 dimensões
embedding_path = 'skip_s50.txt'

# Carregar o modelo de embeddings
print("\nCarregando o modelo de embeddings...")
word_vectors = KeyedVectors.load_word2vec_format(embedding_path)

# Verificar o tamanho dos vetores de embeddings
embedding_dim = word_vectors.vector_size
print(f"Dimensão dos embeddings: {embedding_dim}")

# 3. Representar cada avaliação como um vetor de embeddings

def get_embedding_vector(text):
    tokens = nltk.word_tokenize(text)
    # Lista para armazenar os embeddings das palavras presentes no vocabulário
    vectors = []
    for token in tokens:
        if token in word_vectors:
            vectors.append(word_vectors[token])
    if vectors:
        # Média dos vetores das palavras para representar o texto
        return np.mean(vectors, axis=0)
    else:
        # Se nenhuma palavra estiver no vocabulário, retorna um vetor de zeros
        return np.zeros(embedding_dim)

# Aplicar a função aos textos
print("\nGerando embeddings para os textos...")
df['embedding'] = df['clean_review'].apply(get_embedding_vector)

# Converter a coluna de embeddings em um array numpy
X = np.vstack(df['embedding'].values)
y = df['sentiment']

# 4. Dividir os dados em conjuntos de treinamento e teste

# Dividir em treinamento e teste (80% treinamento, 20% teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 5. Treinar o classificador SVM

# Inicializar o classificador SVM (usando kernel linear para velocidade)
svm_classifier = SVC(kernel='linear', random_state=42)

# Treinar o modelo
print("\nTreinando o modelo SVM...")
svm_classifier.fit(X_train, y_train)

# 6. Avaliar o modelo no conjunto de teste

# Prever os sentimentos no conjunto de teste
y_pred = svm_classifier.predict(X_test)

# Calcular a acurácia
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAcurácia no conjunto de teste: {accuracy:.4f}")

# Exibir o relatório de classificação
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred, digits=4))

# Matriz de Confusão
conf_matrix = confusion_matrix(y_test, y_pred, labels=['positivo', 'neutro', 'negativo'])

# Visualizar a matriz de confusão
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Positivo', 'Neutro', 'Negativo'], yticklabels=['Positivo', 'Neutro', 'Negativo'])
plt.title('Matriz de Confusão')
plt.xlabel('Predito')
plt.ylabel('Verdadeiro')
plt.show()
```

### **Explicação do Código**

#### **1. Carregar os dados pré-processados**

- **Leitura do arquivo CSV**: Carregamos o DataFrame a partir do arquivo `avaliacoes_preprocessadas.csv`.
- **Verificação de valores nulos**: Checamos se há valores nulos e os removemos se necessário.

#### **2. Carregar o modelo de embeddings pré-treinado**

- **Download dos Embeddings**:
  - Baixe o modelo de embeddings em português, como o **skip_s50.txt**, disponível no [NILC](http://nilc.icmc.usp.br/embeddings).
- **Carregar o Modelo**:
  - Usamos `KeyedVectors.load_word2vec_format` para carregar o modelo no formato Word2Vec.
- **Obter a Dimensão dos Embeddings**:
  - `word_vectors.vector_size` retorna a dimensão dos vetores (por exemplo, 50).

#### **3. Representar cada avaliação como um vetor de embeddings**

- **Função `get_embedding_vector`**:
  - Tokeniza o texto e obtém os embeddings de cada palavra.
  - Calcula a média dos vetores das palavras presentes no vocabulário para representar o texto.
  - Se nenhuma palavra estiver no vocabulário, retorna um vetor de zeros.
- **Aplicar a Função aos Textos**:
  - Geramos um vetor de embeddings para cada avaliação e armazenamos na coluna `embedding`.
- **Preparar os Dados**:
  - Convertemos a coluna de embeddings em um array numpy (`X`).
  - `y` contém os rótulos de sentimento.

#### **4. Dividir os dados em conjuntos de treinamento e teste**

- **Divisão dos Dados**:
  - Usamos `train_test_split` para dividir `X` e `y`.

#### **5. Treinar o classificador SVM**

- **Inicialização do SVM**:
  - Usamos o kernel linear.
- **Treinamento do Modelo**:
  - Treinamos o modelo com `X_train` e `y_train`.

#### **6. Avaliar o modelo no conjunto de teste**

- **Predição no Conjunto de Teste**:
  - Usamos o modelo treinado para prever os sentimentos em `X_test`.
- **Cálculo da Acurácia**:
  - Comparamos as predições com os valores verdadeiros (`y_test`).
- **Relatório de Classificação**:
  - Obtemos métricas como precisão, recall e F1-score.
- **Matriz de Confusão**:
  - Visualizamos a performance do modelo em cada classe.

### **Preparação do Ambiente**

- **Download dos Embeddings**:
  - Acesse [NILC Embeddings](http://nilc.icmc.usp.br/embeddings) e baixe o arquivo **skip_s50.zip**.
  - Extraia o arquivo e coloque o arquivo `skip_s50.txt` no diretório do seu projeto.
- **Instalar o Gensim**:
  - Certifique-se de que o `gensim` está instalado:
    ```bash
    pip install gensim
    ```

### **Observações Importantes**

- **Compatibilidade de Palavras**:
  - Nem todas as palavras das avaliações podem estar presentes no vocabulário dos embeddings.
  - A função `get_embedding_vector` lida com isso ao ignorar palavras ausentes.
- **Dimensão dos Embeddings**:
  - Modelos com dimensões maiores podem capturar mais nuances semânticas, mas podem aumentar o tempo de treinamento.
- **Velocidade**:
  - O processo de gerar embeddings pode ser demorado para conjuntos de dados grandes.

### **Possíveis Melhorias**

- **Utilizar Embeddings Contextuais**:
  - Modelos como **BERT** ou **FastText** podem oferecer embeddings mais ricos.
  - O FastText lida melhor com palavras fora do vocabulário ao considerar subpalavras.
- **Ajuste de Hiperparâmetros**:
  - Ajustar os parâmetros do SVM, como o `C`, pode melhorar o desempenho.
- **Dimensionalidade**:
  - Se necessário, aplicar **redução de dimensionalidade** (por exemplo, PCA) antes de treinar o modelo.

### **Próximos Passos**

- **Comparar com o Modelo Anterior**:
  - Compare o desempenho deste modelo com o SVM + BoW.
- **Treinar o Modelo BERT**:
  - Implementar a próxima tarefa usando o BERT para classificação.
- **Análise de Erros**:
  - Identificar onde o modelo está errando para possíveis melhorias.

### **Conclusão**

O uso de embeddings de palavras permite capturar relações semânticas entre as palavras, potencialmente melhorando o desempenho do classificador em relação ao uso de Bag-of-Words. É importante analisar os resultados e interpretar as métricas para avaliar se essa abordagem oferece benefícios significativos para o seu conjunto de dados.

Se tiver alguma dúvida ou precisar de assistência adicional, estou à disposição para ajudar!
// File: content/interactions/3_3-treinamento-BERT.txt

Certamente! Vamos realizar a tarefa de treinar um modelo **BERT** para classificação de sentimento em seu conjunto de dados pré-processado.

### **Objetivo**

Utilizar um modelo BERT pré-treinado e ajustá-lo (fine-tuning) para a tarefa de classificação de sentimento em português.

### **Passos a Seguir**

1. **Carregar os dados pré-processados** do arquivo CSV.
2. **Preparar os dados para o BERT**:
   - Tokenizar o texto usando o tokenizador BERT.
   - Converter os rótulos para formato numérico.
   - Criar um Dataset e DataLoader do PyTorch.
3. **Configurar o modelo BERT para classificação**.
4. **Treinar o modelo**.
5. **Avaliar o modelo** no conjunto de teste.
6. **Exibir os resultados**.

### **Instalação das Bibliotecas Necessárias**

Certifique-se de ter as seguintes bibliotecas instaladas:

```bash
pip install transformers datasets torch scikit-learn
```

### **Código Completo**

```python
# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Verificar se CUDA está disponível
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Usando dispositivo: {device}')

# 1. Carregar os dados pré-processados
df = pd.read_csv('avaliacoes_preprocessadas.csv')

# Exibir as primeiras linhas do DataFrame
print("Dados pré-processados:")
print(df.head())

# Remover linhas com valores nulos (se houver)
df.dropna(inplace=True)

# 2. Preparar os dados para o BERT

# Definir o tokenizador
tokenizer = BertTokenizerFast.from_pretrained('neuralmind/bert-base-portuguese-cased')

# Mapear os rótulos de sentimento para valores numéricos
label_dict = {'negativo': 0, 'neutro': 1, 'positivo': 2}
df['label'] = df['sentiment'].map(label_dict)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(
    df['review_text'],  # Usar o texto original para melhor tokenização
    df['label'],
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)

# Resetar os índices
X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)

# Tokenizar os dados
max_length = 128  # Ajuste conforme necessário

train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length)
test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=max_length)

# Criar uma classe Dataset personalizada
class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels.reset_index(drop=True)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

# Criar os datasets
train_dataset = SentimentDataset(train_encodings, y_train)
test_dataset = SentimentDataset(test_encodings, y_test)

# Criar os DataLoaders
batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 3. Configurar o modelo BERT para classificação

model = BertForSequenceClassification.from_pretrained(
    'neuralmind/bert-base-portuguese-cased',
    num_labels=3  # Três rótulos: negativo, neutro, positivo
)

model.to(device)

# 4. Treinar o modelo

# Configurar o otimizador e scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 3
total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# Definir a função de perda
loss_fn = nn.CrossEntropyLoss()

# Loop de treinamento
for epoch in range(epochs):
    print(f'Época {epoch + 1}/{epochs}')
    model.train()
    total_train_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        total_train_loss += loss.item()

        loss.backward()
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_loader)
    print(f'Perda média de treinamento: {avg_train_loss:.4f}')

    # Validação
    model.eval()
    total_eval_loss = 0
    predictions, true_labels = [], []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_eval_loss += loss.item()

            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    avg_val_loss = total_eval_loss / len(test_loader)
    print(f'Perda média de validação: {avg_val_loss:.4f}')

    # Calcular acurácia
    accuracy = np.sum(np.array(predictions) == np.array(true_labels)) / len(true_labels)
    print(f'Acurácia na validação: {accuracy:.4f}')

# 5. Avaliar o modelo no conjunto de teste

print("\nRelatório de Classificação:")
target_names = ['negativo', 'neutro', 'positivo']
print(classification_report(true_labels, predictions, target_names=target_names, digits=4))

# Matriz de Confusão
conf_matrix = confusion_matrix(true_labels, predictions)

# Plotar a matriz de confusão
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Matriz de Confusão')
plt.xlabel('Predito')
plt.ylabel('Verdadeiro')
plt.show()
```

### **Explicação do Código**

#### **Importação das Bibliotecas**

Importamos as bibliotecas necessárias, incluindo `torch`, `transformers` e módulos do `sklearn` para construção, treinamento e avaliação do modelo.

#### **Carregando os Dados Pré-processados**

Carregamos os dados do arquivo `avaliacoes_preprocessadas.csv` e garantimos que não haja valores nulos.

#### **Preparando os Dados para o BERT**

- **Tokenizador**: Utilizamos o `BertTokenizerFast` para tokenizar os dados de texto. Como os dados estão em português, usamos o modelo pré-treinado `neuralmind/bert-base-portuguese-cased`.

- **Mapeamento de Rótulos**: Convertendo os rótulos de sentimento para valores numéricos:

  ```python
  label_dict = {'negativo': 0, 'neutro': 1, 'positivo': 2}
  ```

- **Divisão dos Dados**: Dividimos os dados em conjuntos de treinamento e teste, garantindo a estratificação para manter o equilíbrio das classes.

- **Tokenização**: Tokenizamos os textos utilizando o tokenizador, com truncamento e padding para um comprimento máximo (por exemplo, 128 tokens).

- **Dataset e DataLoader**: Criamos uma classe personalizada `SentimentDataset` e usamos `DataLoader` para facilitar o batching e shuffling durante o treinamento.

#### **Configurando o Modelo BERT**

Carregamos o modelo BERT pré-treinado para classificação de sequência, especificando o número de rótulos como 3. Movemos o modelo para o dispositivo apropriado (GPU se disponível).

#### **Treinando o Modelo**

- **Otimizador e Scheduler**: Utilizamos o otimizador `AdamW` e um scheduler de taxa de aprendizado linear com warm-up.

- **Função de Perda**: Usamos `CrossEntropyLoss` como a função de perda.

- **Loop de Treinamento**: Para cada época, treinamos o modelo, calculamos a perda de treinamento e realizamos a validação para calcular a perda e a acurácia de validação.

#### **Avaliando o Modelo**

Após o treinamento, exibimos o relatório de classificação e plotamos a matriz de confusão para avaliar o desempenho do modelo no conjunto de teste.

### **Notas Importantes**

- **Modelo Pré-treinado**: Utilizamos o modelo `neuralmind/bert-base-portuguese-cased`, que é um BERT pré-treinado em um grande corpus de dados em português.

- **Uso de GPU**: O treinamento de modelos BERT pode ser intensivo computacionalmente. Recomenda-se o uso de GPU para acelerar o processo. Verifique se você tem acesso a uma GPU compatível e se o PyTorch está instalado com suporte a CUDA.

- **Ajuste de Hiperparâmetros**: Você pode experimentar diferentes valores para `batch_size`, `epochs`, `learning_rate` e `max_length` para ver como eles afetam o desempenho do modelo.

- **Tratamento de Desbalanceamento de Classes**: Se o seu conjunto de dados for desbalanceado, considere técnicas como ponderação da perda ou oversampling das classes minoritárias.

### **Próximos Passos**

- **Salvar e Carregar o Modelo**: Você pode salvar o modelo treinado para uso futuro:

  ```python
  # Salvar o modelo
  model.save_pretrained('modelo_bert_sentimento')
  tokenizer.save_pretrained('modelo_bert_sentimento')

  # Carregar o modelo
  model = BertForSequenceClassification.from_pretrained('modelo_bert_sentimento')
  tokenizer = BertTokenizerFast.from_pretrained('modelo_bert_sentimento')
  ```

- **Testar em Novos Dados**: Use o modelo treinado para prever sentimentos em novos dados não vistos.

- **Comparar com Outros Modelos**: Compare o desempenho do modelo BERT com os modelos anteriores (SVM + BoW, SVM + Embeddings) em termos de acurácia, F1-score e outras métricas relevantes.

- **Ajuste Fino**: Experimente técnicas de fine-tuning mais avançadas, como congelar certas camadas ou ajustar a taxa de aprendizado de forma diferenciada para diferentes partes do modelo.

### **Conclusão**

O treinamento de um modelo BERT para classificação de sentimento pode melhorar significativamente o desempenho devido à capacidade do BERT de capturar padrões linguísticos complexos. Certifique-se de pré-processar seus dados adequadamente e ajustar o modelo conforme necessário para obter os melhores resultados.

Se tiver alguma dúvida ou precisar de assistência adicional em qualquer etapa, estou à disposição para ajudar!
// File: content/interactions/4-analise-de-resultados

Claro! Vamos gerar o código para comparar a performance e os resultados entre os três classificadores que você treinou:

1. **SVM + Bag-of-Words (BoW)**
2. **SVM + Embeddings**
3. **BERT**

O objetivo é avaliar e comparar as métricas de desempenho de cada modelo, incluindo acurácia, precisão, recall e F1-score, além de visualizar as matrizes de confusão.

### **Passos a Seguir**

1. **Consolidar as Predições e Rótulos Verdadeiros**: Garantir que temos as predições (`y_pred`) e os rótulos verdadeiros (`y_test`) de cada modelo.

2. **Calcular as Métricas de Desempenho**: Para cada modelo, calcular acurácia, precisão, recall e F1-score.

3. **Organizar os Resultados**: Criar uma tabela (DataFrame) que resume as métricas de cada modelo.

4. **Visualizar as Matrizes de Confusão**: Plotar as matrizes de confusão lado a lado para comparar o desempenho em cada classe.

### **Código Completo**

```python
# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Definir nomes dos modelos
model_names = ['SVM + BoW', 'SVM + Embeddings', 'BERT']

# Assegurar que temos as predições e rótulos verdadeiros de cada modelo
# Supondo que você tenha as seguintes variáveis a partir dos códigos anteriores:
# Para SVM + BoW:
# - y_test_bow: rótulos verdadeiros
# - y_pred_bow: predições do modelo

# Para SVM + Embeddings:
# - y_test_embed: rótulos verdadeiros
# - y_pred_embed: predições do modelo

# Para BERT:
# - true_labels_bert: rótulos verdadeiros
# - predictions_bert: predições do modelo

# Se essas variáveis não estão definidas, você precisa executar os códigos de treinamento novamente ou salvar os resultados.

# Para fins deste exemplo, vamos supor que as variáveis estão disponíveis.

# 1. Consolidar as predições e rótulos verdadeiros

# Converter rótulos para valores numéricos (se necessário)
label_dict = {'negativo': 0, 'neutro': 1, 'positivo': 2}

# SVM + BoW
# Se y_test e y_pred estão em formato de string, converter para numérico
y_test_bow_numeric = y_test.map(label_dict) if y_test.dtype == 'object' else y_test
y_pred_bow_numeric = pd.Series(y_pred).map(label_dict) if isinstance(y_pred[0], str) else y_pred

# SVM + Embeddings
y_test_embed_numeric = y_test.map(label_dict) if y_test.dtype == 'object' else y_test
y_pred_embed_numeric = pd.Series(y_pred_embed).map(label_dict) if isinstance(y_pred_embed[0], str) else y_pred_embed

# BERT
# As predições e rótulos já estão em formato numérico
y_test_bert = true_labels  # já numérico
y_pred_bert = predictions  # já numérico

# 2. Calcular as métricas de desempenho

# Criar um DataFrame para armazenar as métricas
metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

# Função para calcular as métricas
def calculate_metrics(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    metrics_df.loc[len(metrics_df)] = [model_name, accuracy, precision, recall, f1]

# Calcular métricas para cada modelo
calculate_metrics(y_test_bow_numeric, y_pred_bow_numeric, 'SVM + BoW')
calculate_metrics(y_test_embed_numeric, y_pred_embed_numeric, 'SVM + Embeddings')
calculate_metrics(y_test_bert, y_pred_bert, 'BERT')

# Exibir o DataFrame com as métricas
print("Comparação das Métricas de Desempenho:")
print(metrics_df)

# 3. Visualizar as Matrizes de Confusão

# Definir os rótulos das classes
target_names = ['negativo', 'neutro', 'positivo']

# Criar subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# SVM + BoW
conf_matrix_bow = confusion_matrix(y_test_bow_numeric, y_pred_bow_numeric)
sns.heatmap(conf_matrix_bow, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names, ax=axes[0])
axes[0].set_title('Matriz de Confusão - SVM + BoW')
axes[0].set_xlabel('Predito')
axes[0].set_ylabel('Verdadeiro')

# SVM + Embeddings
conf_matrix_embed = confusion_matrix(y_test_embed_numeric, y_pred_embed_numeric)
sns.heatmap(conf_matrix_embed, annot=True, fmt='d', cmap='Greens', xticklabels=target_names, yticklabels=target_names, ax=axes[1])
axes[1].set_title('Matriz de Confusão - SVM + Embeddings')
axes[1].set_xlabel('Predito')
axes[1].set_ylabel('Verdadeiro')

# BERT
conf_matrix_bert = confusion_matrix(y_test_bert, y_pred_bert)
sns.heatmap(conf_matrix_bert, annot=True, fmt='d', cmap='Oranges', xticklabels=target_names, yticklabels=target_names, ax=axes[2])
axes[2].set_title('Matriz de Confusão - BERT')
axes[2].set_xlabel('Predito')
axes[2].set_ylabel('Verdadeiro')

plt.tight_layout()
plt.show()
```

### **Explicação do Código**

#### **1. Consolidar as Predições e Rótulos Verdadeiros**

- **Conversão de Rótulos**: Garantimos que os rótulos e predições estão no mesmo formato (numérico) para todos os modelos.
- **Mapeamento de Rótulos**: Usamos `label_dict` para converter rótulos de string para valores numéricos, se necessário.

#### **2. Calcular as Métricas de Desempenho**

- **Função `calculate_metrics`**: Calcula acurácia, precisão, recall e F1-score para um conjunto de predições e rótulos verdadeiros.
- **Criação do DataFrame `metrics_df`**: Armazena as métricas de cada modelo para fácil comparação.
- **Métricas Ponderadas**: Usamos `average='weighted'` para levar em conta o desbalanceamento de classes.

#### **3. Visualizar as Matrizes de Confusão**

- **Subplots**: Criamos um gráfico com três subplots lado a lado, um para cada modelo.
- **Matrizes de Confusão**: Usamos `confusion_matrix` e `seaborn.heatmap` para visualizar a performance em cada classe.
- **Personalização**: Definimos títulos, rótulos dos eixos e cores diferentes para cada matriz para melhor visualização.

### **Resultados Esperados**

- **Tabela de Métricas**: Uma tabela mostrando acurácia, precisão, recall e F1-score de cada modelo.
- **Matrizes de Confusão**: Gráficos que permitem comparar visualmente o desempenho dos modelos em cada classe.

### **Interpretação dos Resultados**

- **Acurácia**: Proporção de predições corretas sobre o total de exemplos.
- **Precisão**: Proporção de predições positivas corretas em relação ao total de predições positivas feitas.
- **Recall**: Proporção de verdadeiros positivos que foram corretamente identificados.
- **F1-Score**: Média harmônica entre precisão e recall, útil quando há desbalanceamento de classes.
- **Matrizes de Confusão**: Mostram o número de acertos e erros para cada classe, permitindo identificar padrões de erros.

### **Análise Comparativa**

- **Desempenho Geral**: Qual modelo teve a melhor acurácia e F1-score?
- **Desempenho por Classe**: Algum modelo tem dificuldade em classificar uma determinada classe?
- **Equilíbrio de Métricas**: O modelo com maior acurácia também possui alta precisão e recall?

### **Possíveis Melhorias**

- **Análise de Erros**: Examinar exemplos onde os modelos erraram para entender possíveis causas.
- **Ajuste de Hiperparâmetros**: Realizar otimização de hiperparâmetros para melhorar o desempenho dos modelos.
- **Conjunto de Validação**: Usar um conjunto de validação separado para evitar overfitting durante o ajuste de hiperparâmetros.
- **Ensemble de Modelos**: Combinar as predições dos modelos para potencialmente melhorar o desempenho.

### **Conclusão**

A comparação entre os modelos permite identificar qual abordagem é mais eficaz para a tarefa de classificação de sentimento em seu conjunto de dados. O modelo BERT, por exemplo, pode apresentar melhor desempenho devido à sua capacidade de capturar contextos complexos na linguagem natural.

### **Notas Finais**

- **Reprodutibilidade**: Certifique-se de que todas as variáveis necessárias (predições e rótulos) estão disponíveis antes de executar o código.
- **Visualização**: Ajuste os tamanhos e estilos dos gráficos conforme necessário para melhor apresentação.
- **Documentação**: Inclua esses resultados em sua apresentação, destacando insights importantes e possíveis razões para as diferenças observadas entre os modelos.

Se precisar de assistência adicional ou tiver dúvidas sobre qualquer parte do código ou interpretação dos resultados, estou aqui para ajudar!
// File: content/interactions/5-BONUS-in-content-learning.txt

Claro! Vamos implementar a classificação de sentimento utilizando **in-context learning** com **LLMs** conforme o requisito bônus.

### **Compreendendo o In-Context Learning**

O in-context learning permite que modelos de linguagem grandes (LLMs) realizem tarefas específicas fornecendo exemplos diretamente no prompt, sem a necessidade de treinamento adicional. Podemos aproveitar modelos como o GPT-3 para classificar sentimentos fornecendo alguns exemplos anotados (few-shot learning).

### **Abordagem**

1. **Configurar o Ambiente**: Instalar bibliotecas necessárias e configurar a chave da API da OpenAI.
2. **Preparar os Dados**: Utilizar os dados de teste pré-processados das etapas anteriores.
3. **Definir o Prompt**: Criar um prompt que inclua exemplos de avaliações com seus respectivos sentimentos.
4. **Criar uma Função**: Definir uma função para enviar requisições à API da OpenAI e receber as predições.
5. **Classificar os Dados de Teste**: Utilizar a função para classificar o sentimento das avaliações de teste.
6. **Avaliar o Modelo**: Comparar as predições com os rótulos verdadeiros e calcular métricas como acurácia e F1-score.

### **Implementação**

#### **1. Configurar o Ambiente**

```python
# Instalar a biblioteca OpenAI se ainda não estiver instalada
!pip install openai

import openai
import os
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
```

**Nota:** Certifique-se de ter uma chave de API da OpenAI. Você pode obtê-la no [site da OpenAI](https://platform.openai.com/account/api-keys). **Não compartilhe sua chave de API publicamente.**

```python
# Defina sua chave de API da OpenAI
openai.api_key = "SUA_CHAVE_DE_API_AQUI"
```

#### **2. Preparar os Dados**

Usaremos os dados de teste pré-processados (`X_test` e `y_test`) das etapas anteriores. Certifique-se de que essas variáveis estão disponíveis.

```python
# Se ainda não carregou, carregue os dados pré-processados
df = pd.read_csv('avaliacoes_preprocessadas.csv')

# Mapear os rótulos de sentimento para strings, se estiverem em formato numérico
label_dict = {'negativo': 0, 'neutro': 1, 'positivo': 2}
label_dict_inv = {v: k for k, v in label_dict.items()}

# Usar a mesma divisão de treino e teste de antes
from sklearn.model_selection import train_test_split

X = df['review_text']  # Usar o texto original para melhor contexto
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Resetar os índices
X_test = X_test.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)
```

#### **3. Definir o Prompt**

Precisamos criar um prompt que inclua vários exemplos de avaliações com seus sentimentos correspondentes.

```python
# Criar exemplos para o prompt
examples = [
    {'text': 'O produto é excelente, superou minhas expectativas.', 'sentiment': 'positivo'},
    {'text': 'Não gostei do material, parece ser frágil.', 'sentiment': 'negativo'},
    {'text': 'É razoável pelo preço que paguei.', 'sentiment': 'neutro'},
    {'text': 'Atendimento péssimo, não recomendo.', 'sentiment': 'negativo'},
    {'text': 'Entrega rápida e produto de alta qualidade!', 'sentiment': 'positivo'},
    {'text': 'O produto é bom, mas a embalagem chegou danificada.', 'sentiment': 'neutro'},
]
```

#### **4. Criar uma Função para Classificar o Texto**

Definiremos uma função que envia uma requisição à API da OpenAI para cada avaliação.

```python
def classify_sentiment(text, examples):
    # Construir o prompt
    prompt = "Classifique o sentimento das seguintes avaliações em 'positivo', 'neutro' ou 'negativo'.\n\n"
    for ex in examples:
        prompt += f"Avaliação: {ex['text']}\nSentimento: {ex['sentiment']}\n\n"
    prompt += f"Avaliação: {text}\nSentimento:"

    # Chamar a API da OpenAI
    response = openai.Completion.create(
        engine="text-davinci-003",  # Você pode experimentar com "gpt-3.5-turbo" ou outro modelo
        prompt=prompt,
        max_tokens=1,
        temperature=0,
        top_p=1,
        n=1,
        stop=["\n"],
    )

    sentiment = response.choices[0].text.strip().lower()
    return sentiment
```

#### **5. Classificar os Dados de Teste**

Aplicaremos a função aos dados de teste. Note que chamar a API da OpenAI para cada avaliação pode ser demorado e pode incorrer em custos.

```python
# Limitar o número de amostras para fins de demonstração (por exemplo, primeiras 100 amostras)
num_samples = 100
X_test_sample = X_test[:num_samples]
y_test_sample = y_test[:num_samples]

# Classificar as amostras
predictions = []
for review in X_test_sample:
    sentiment = classify_sentiment(review, examples)
    predictions.append(sentiment)
```

#### **6. Avaliar o Modelo**

Mapear os rótulos verdadeiros e predições para valores numéricos para avaliação.

```python
# Mapear os rótulos verdadeiros para valores numéricos
y_test_numeric = y_test_sample.map(label_dict).values

# Mapear as predições para valores numéricos
y_pred_numeric = [label_dict.get(sentiment, -1) for sentiment in predictions]

# Remover quaisquer predições que não puderam ser mapeadas
valid_indices = [i for i, label in enumerate(y_pred_numeric) if label != -1]
y_test_valid = y_test_numeric[valid_indices]
y_pred_valid = [y_pred_numeric[i] for i in valid_indices]

# Calcular as métricas de avaliação
accuracy = accuracy_score(y_test_valid, y_pred_valid)
print(f"\nAcurácia no conjunto de teste: {accuracy:.4f}")

print("\nRelatório de Classificação:")
target_names = ['negativo', 'neutro', 'positivo']
print(classification_report(y_test_valid, y_pred_valid, target_names=target_names, digits=4))

# Plotar a matriz de confusão
conf_matrix = confusion_matrix(y_test_valid, y_pred_valid)
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Matriz de Confusão - In-Context Learning com LLM')
plt.xlabel('Predito')
plt.ylabel('Verdadeiro')
plt.show()
```

### **Notas**

- **Custos da API**: Esteja ciente de que utilizar a API da OpenAI pode gerar custos. Verifique seu uso da API e configure limites adequados.
- **Limites de Taxa**: A API possui limites de taxa. Se você estiver processando um grande número de amostras, pode ser necessário implementar atrasos ou processamento em lote.
- **Seleção de Modelo**: Você pode experimentar diferentes modelos como `gpt-3.5-turbo` ou `text-davinci-003`.
- **Tratamento de Erros**: Implemente tratamento de erros para gerenciar erros da API ou respostas inesperadas.

### **Conclusão**

Implementamos a classificação de sentimento utilizando in-context learning com um LLM. Esta abordagem aproveita o poder dos modelos de linguagem grandes sem a necessidade de treinamento adicional, fornecendo exemplos diretamente no prompt.

### **Comparação com Outros Modelos**

Agora você pode incluir este modelo em sua comparação junto com os modelos SVM e BERT, avaliando o desempenho e analisando os resultados.
