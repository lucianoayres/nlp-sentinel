// File: content/slides/Aula 0 - Intro.txt

Processamento de Linguagem Natural
Introdução ao Curso
Prof. Luciano Barbosa &
Prof. Johny Moreira
{luciano, jms5}@cin.ufpe.br

O que é PLN
Wikipedia
https://www.slideshare.net/pranavgupta21/introduction-to-natural-language-processing-13847262
2

https://www.youtube.com/watch?v=ARJ8cAGm6JE
3

https://www.slideshare.net/minhpqn/introduction-to-natural-language-processing-67212472
4

Motivação para o Uso de PLN
Linguagem é complexa e envolve várias atividades humanas
Leitura, escrita, fala, audição
Desejo de extrair conhecimento de textos
Artigos científicos, notícias etc
https://www.slideshare.net/minhpqn/introduction-to-natural-language-processing-67212472
5

Aplicações
https://datasciencedojo.com/blog/natural-language-processing-applications/

Aplicações: Tradução

Aplicações: Reconhecimento de Fala

Aplicações: Predição de Próxima Palavra

Desafio: Ambiguidade
Ex: I made her duck 
Significados:

Desafio: Ambiguidade

A Disciplina
Objetivo:
Aprender principais técnicas, tarefas e aplicações de Processamento de Linguagem Natural
Avaliação: projeto


Projeto: Implementar Análise de Sentimentos em Avaliações de Produtos
Coletar avaliações de um produto selecionado: texto e nota
Treinar classificador: SVM + bow, SVM + embeddings, BERT
Bônus: utilizar in-context learning para a classificação
Apresentação reportando resultados (F1 e acurácia) e análises (slides + vídeo de máximo 15 minutos)
Pré-Processamento
Treinamento do Classificador
Avaliação
Avaliações

Bibliografia
Speech and Language Processing (3rd ed. draft). Dan Jurafsky and James H. Martin
Foundations of Statistical Natural Language Processing. Christopher D. Manning and Hinrich Schütze
Natural Language Processing. Jacob Eisenstein
Lin, Tianyang, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. "A survey of transformers." AI Open (2022).

Tópicos
Processamento de texto
Representação de palavras
Extração de informação
Modelos de linguagem 
Geração automática de texto 
Recuperação de informação




Processamento de Texto: Segmentação de Palavras
Primeira tarefa em um sistema de PLN




Estado da arte utiliza machine learning
Obtêm alta acurácia


Classificação de Texto: Análise de Sentimentos

Representação de Palavras: Embeddings

Extração de Informação

Chatbot: ELIZA

Modelo de Linguagem

Machine Translation

Recuperação de Informação

Geração de Texto

// File: content/slides/Aula 1 - Conceitos Introdutórios e Processamento de Texto.txt

Processamento de Linguagem Natural
Processamento de Texto
Prof. Luciano Barbosa &
Prof. Johny Moreira
{luciano, jms5}@cin.ufpe.br

Fatores que Influenciam a Construção de um Texto

 African American English (AAF)

iont
talmbout
Mainstream American English (MAE)
I don’t
talking about
Quem escreveu (idade, gênero, classe social)
Quando foi escrito
Finalidade: 
notícia, artigos científicos, livros de romance, tweets ou postagem na internet
Língua: 7097 línguas no mundo
Combinação de línguas e dialetos
Dialetos regionais
“Code switching”
Por primera vez veo a @username actually being hateful! it was beautiful:)
[Pela primeira vez vejo a @nomedeusuario sendo realmente odiosa! Foi lindo:)]
dost tha or ra- hega ... dont wory ... but dherya rakhe
[Ele foi e continuará sendo um amigo… não se preocupe… mas tenha fé]

Etapas de Processamento de Texto
Tokenização de palavras
Normalização de palavras
Segmentação de sentenças


Tokenização
Quebrar sequência de caracteres em palavras
Exemplo de abordagens simples:
Qualquer sequência de caracteres alfanuméricos de tamanho mínimo 3
Terminado em espaço
Terminado em algum caracter especial
Bigcorp’s 2007 bi-annual report showed profits rose 10%
["bigcorp","2007","annual","report","showed","profits","rose"]

Tokenização
Quebrar sequência de caracteres em palavras
Exemplo de abordagens simples:
Qualquer sequência de caracteres alfanuméricos de tamanho mínimo 3
Terminado em espaço
Terminado em algum caracter especial
Mr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t amusing
["mr","neill","thinks","that","the","boys","stories","about","chile","capital","aren","amusing"]

Tokenização: Dificuldades
Palavras pequenas podem ser importantes para o significado
Ex: am, pm, el (paso), (world war) II
Hífens
Algumas vezes são necessários
Ex: e-bay, cd-rom, t-shirts, guarda-chuva, bem-te-vi
Palavras separadas
Ex.: Dallas-Fort Worth, spanish-speaking

Tokenização: Dificuldades
Caracteres especiais são importantes para URL, tags, preços e código em documentos
AT&T, R$45.55, 01/02/06, #NLPRules, usuario@gmail.com
Apóstrofo pode ser parte de uma palavra ou parte de um possessivo (inglês)

Tokenização: Dificuldades
Números podem ser importantesEx: nokia 3250, united 93, quicktime 6.5 pro
Pontos podem estar em números, abreviações, URLs, fim de sentenças etcEx: I.B.M., Ph.D., U.S.A., N.S.A

Tokenização: Dificuldades
Linguagens como Chinês, Japonês e Tailandês que não usam espaços para demarcar limites de palavras
Cada caractere representa uma unidade de significado (morfema)
Decidir o que é uma palavra em linguagens do tipo é mais complexo
Texto em chinês

Tokenização: Dificuldades
Texto em japonês

Tokenização: Dificuldades
姚明进入总决赛 
(“Yao Ming reaches the finals”)
姚明       进入      总决赛
YaoMing      reaches         finals
姚    明    进入     总      决赛
Yao    Ming      reaches     overall       finals
姚    明    进    入     总       决     赛
Yao    Ming     enter   enter     overall       decision    game
1 - Segmentação Chinese Treebank 
(3 palavras)
2 - Segmentação Peking University
(5 palavras)
3 - Segmentação por caracteres
(7 palavras)

Tokenização: Ambiguidade
Fonte: https://nlp.stanford.edu/IR-book/ - Capítulo 2 - The term vocabulary & postings lists

Tokenização: Substantivos compostos sem espaços
computerlinguistik
computer + linguistik
computational linguistics
lebensversicherungsgesellschaftsangestellter
leben + versicherung + gesellschaft + angestellter
life insurance company employee
Exemplos em Alemão:

Tokenização: Morfologia complexa
Palavra Turca: 
Uygarlastiramadiklarimizdanmissinizcasina
“(behaving) as if you are among those whom we could not civilize”

Uygar “civilized” + las “become” + tir “cause” + ama “not able” 
+ dik “past” + lar “plural” + imiz “p1pl” + 
dan “abl” + mis “past” + siniz “2pl” + casina “as if”

Normalização
É a tarefa de colocar as palavras/tokens em um formato padrão
Objetivo: compactação do texto
Existe perda de informação 
Exemplo: transformação de caixa (maiúsculas > minúsculas)
Não recomendável em alguns casos: extração de informação e tradução
Sem Transformação
Com Transformação
Peru (País)
peru (ave)
UNE (organização)
une (unir)
US (País)
us ( nós em inglês)
Apple (empresa)
apple (fruta)

Normalização: Stopwords
Palavras que aparecem muito na coleção
Não possuem um significado bem definido
Usualmente, não são boas para diferenciar
Artigos, preposições, conjunções etc
Criadas a partir de palavras com alta frequência ou de listas existentes
Ex: Stopwords em Português (https://gist.github.com/alopes/5358189)
Fonte: https://nlp.stanford.edu/IR-book/ - Capítulo 2 - The term vocabulary & postings lists

Normalização: Lematização
Agrupar palavras com mesma raiz (lemma)
Verbos no passado são colocados no presente
Sinônimos são unificados
foi → vai
melhor → bom
correr, corre, correu → correr
quero, queres, quereres → querer
am, are, is → be
car, cars, car's, cars' → car
He is reading detective stories → He be read detective story

Normalização: Stemming
Reduz variações morfológicas das palavras para um stem em comum
Remove prefixo ou sufixo -> stem
Exemplo 
Palavras: connected, connecting, connection, connections
Stem: connect
Não há um consenso sobre benefícios (depende da língua/tarefa)

Stemming: Porter Stemmer
Um dos mais utilizados
Consiste de uma série de regras executadas em cascata
Comete erros e difícil de modificar
Exemplo de regras

Efeito de Normalização no Corpus Reuters-RCV1

Segmentação de Sentenças
Caracteres ! e ? são precisos para separar
Caracter “.” nem tanto
Abreviação: Dr.
Número: 7.5%
Estratégia mais usada:
Toqueniza primeiro
Usa regras ou Machine Learning para classificar o ponto
Uso de dicionário de abreviação

Ferramentas e Bibliotecas
Stanford tokenizer para inglês (http://nlp.stanford.edu/software/tokenizer.shtml)
CoreNLP (https://stanfordnlp.github.io/CoreNLP/)
NLTK (https://www.nltk.org/)
Spacy (https://spacy.io/)

Aula Prática
Google Colab

Exercício Proposto
Google Colab

Referências
Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/2.pdf>. Capítulo 2. Acesso em: 01 Setembro de 2022.
Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. Disponível em: <https://nlp.stanford.edu/IR-book/>. Capítulo 2. Acesso em: 01 Setembro de 2022.
Recursos e Ferramentas para a Lingua Portuguesa http://www.nilc.icmc.usp.br/nilc/index.php/tools-and-resources 

// File: content/slides/Aula 2 - Representação de Características Textuais.txt

Processamento de Linguagem Natural
Representação de Características Textuais
Prof. Luciano Barbosa &
Prof. Johny Moreira
{luciano, jms5}@cin.ufpe.br

Motivação
Textos, palavras, suas sintaxes ou significados semânticos não são entendidos por máquinas (computadores)
Necessário transformar o texto em números

Representação Categórica
One-hot encoding

One Hot Encoding
Cada palavra é mapeada para uma dimensão de um vetor
O vetor gerado é a representação da palavra
Abordagem simples
4

One Hot Encoding
Dimensionalidade do tamanho do vocabulário
Palavras similares podem apresentar representações muito diferentes
5
D1 : "o brasil sediou a copa" 
= {[1,0,0,0,0,0],[0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]}
D2 : "o brasil perdeu a copa" 
= [1,0,0,0,0,0],[0,1,0,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]
V: {o, brasil, sediou, perdeu, a, copa}

Codificação 
o: 		[1,0,0,0,0,0]
brasil: 	[0,1,0,0,0,0]
sediou: 	[0,0,1,0,0,0]
perdeu: 	[0,0,0,1,0,0]
a:		[0,0,0,0,1,0]
Copa:	[0,0,0,0,0,1]

Representação Baseada em Pesos
Bag-of-Words

Bag of Words
Não guarda a ordem
Cada documento é representado por um vetor
Dimensionalidade: 
Tamanho do vocabulário
Cada palavra tem um peso
Simples e efetivo
Considera apenas o aspecto léxico das unidades linguísticas
Não modela similaridade semântica
7
D1 : "the cat sat on the hat" 
D2 : "the dog ate the cat and the hat"
V
[the, cat, sat, on, hat, dog, ate, and]
D1
[2,     1,    1,    1,   1,     0,     0,    0]
D2
[3,     1,    0,    0,   1,     1,     1,    1]

Peso dos Termos
Termos em um documento não são igualmente úteis para descrever seu conteúdo
Ex: palavras frequentes no documento -> importantes
Ex: palavras que aparecem em todos documentos da coleção -> não importantes
Peso usado para caracterizar a importância do termo


TF - Term Frequency
A importância do termo em um documento é proporcional à sua frequência em um documento 


TF - Term Frequency
Exemplo utilizando a variação do tf com log


Inverse Document Frequency (IDF)
Medir a especificidade de um termo
O inverso do número de documentos nos quais o termo ocorre




Inverse Document Frequency (IDF)
Exemplo utilizando a variação do idf com log


TFIDF
Combinação do tf com o idf: tf x idf



Aprendizagem de 
Representação de Palavras
Word Embeddings
15

Representation Learning
Em modelos de ML, instâncias são representadas por características
Motivação:
A representação de instância/dados é essencial para modelos de ML eficazes
Menos dependente da engenharia de características
Redução de dimensionalidade
Definição:
Conjunto de técnicas que aprendem uma "melhor" representação dos dados brutos
Representação distribuída ou embeddings
Representação densa e de baixa dimensão
Dimensões não têm significado

16

Exemplo de Representation Learning: MLP
Diferentes níveis de abstração da entrada

17

Aprendizagem de Representação de Palavras:
Word Embeddings
Representam qualquer unidade linguística como um vetor denso
Unidades: caracter, palavra, sentença ou documento
Mapeia o significado semântico em um espaço geométrico (espaço dos embeddings)
O contexto de uma palavra pode ser mapeado em um vetor de baixa dimensionalidade
Palavras similares (em termos de contexto) estão próximas nesse espaço

18

Aprendizagem de Representação de Palavras:
Word Embeddings
Fonte: https://medium.com/@hari4om/word-embedding-d816f643140
19

Técnicas para criação
Modelos baseado em frequência (LSI)
Modelos baseado em predição (redes neurais)
Aprendizagem de Representação de Palavras:
Word Embeddings
20

Word2Vec
CBOW e Skip-gram
21

Word2Vec: CBOW
Fonte: https://thinkinfi.com/continuous-bag-of-words-cbow-multi-word-model-how-it-works/

Prevê a representação de uma palavra, dado o seu contexto 

22

Word2Vec: CBOW
word context
(one-hot encoding)
distribuição de probabilidade das palavras no corpus 
representação 
da palavra
Fonte: https://arxiv.org/pdf/1411.2738v3.pdf
23
Dimensão das entradas e saída: tamanho do vocabulário

N distribuições de probabilidade das palavras no corpus
word
(one-hot encoding)
Dada uma palavra como entrada, prevê o seu contexto.
Word2Vec: Skip-gram
Fonte: https://arxiv.org/pdf/1411.2738v3.pdf
representação 
da palavra
Input word
24

Modelos anteriores são muito custosos
Prevê palavras como vizinhas
Exemplos negativos: palavras que não são vizinhas
Entradas:
Palavra e seu contexto
Saída:
Probabilidade de matching
Word2Vec: Skip-gram with Negative Sampling

Word2Vec: Skip-gram with Negative Sampling
Fonte: https://jalammar.github.io/illustrated-word2vec/

Exemplos de Embeddings
27

Exemplos de Embeddings
28

Exemplos de Embeddings
29

Embeddings
Na maioria das redes para texto, word embeddings é a entrada padrão
Qual é o melhor é difícil dizer:
Use vetores pré-treinados disponíveis (você também pode criar o seu)
Seleção de corpus e ajuste dos parâmetros para a uma dada tarefa:
Por exemplo. Para análise de sentimento, as palavras "bom" e "ruim" devem estar distantes no espaço vetorial
30

Modelos amplamente utilizados
Word2Vec
https://code.google.com/p/word2vec/

Glove
http://nlp.stanford.edu/projects/glove/ 

FastText
https://fasttext.cc/ 

Doc2Vec
https://radimrehurek.com/gensim/models/doc2vec.html 
31

Exemplo para Representação de Anúncios
32

Exemplo para Representação de Anúncios
33

34
Aula Prática
Google Colab

Resumo da Aula
Representação Textual Categórica: one-hot encoding
Representação Textual baseada em Contagem: bag-of-words
Representação baseada em Aprendizagem: Word2Vec
Prática:
Classificação de Textos em Grupos de Notícias (fetch_20newsgroups)
Classificação de Sentimento - predição da contagem de estrelas de uma review de produtos (Amazon Review Corpus - Fashion)
35

36
Exercício Proposto
Google Colab

REFERÊNCIAS
JURAFSKY, Daniel; MARTIN, James H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. https://web.stanford.edu/~jurafsky/slp3/. 
MIKOLOV, Tomas et al. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
PENNINGTON, Jeffrey; SOCHER, Richard; MANNING, Christopher D. Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014. p. 1532-1543.
37

// File: content/slides/Aula 3 - Extração de Informação.txt

Processamento de Linguagem Natural
Extração de Informação

Objetivo
Extrair estrutura a partir de dados não estruturados
Barack Obama was born in Hawaii…
Unstructured Text
Barack Obama
Born
Hawaii
was
was born in
Structured Text
Information Extraction
Nome
Barack Obama
Local de Nascimento
Hawaii
Data de Nascimento
04/08/1961
Profissão
advogado, político
Esposa
Michelle Robinson
Filhos
Malia Ann, Natasha
O que é Extração de Informação? Detectar informações que são importantes dentro de um texto não estruturado.

Dado não estruturado. Texto Corrido. Difícil de ser processado por máquina

Dado estruturado. Texto organizado por segmentos. Ex: formato de grafo. Melhor o entendimento/processamento de máquina.

Pode ajudar em várias tarefas: Consulta, consumo, exploração de relações. Diferenciar pessoas de locais, de organizações… 


Rotulagem Sequencial
Objetivo: atribuir um dado rótulo a cada palavra de um sentença
Rótulos dependem de outras palavras da sequência (não é i.i.d)

Algumas Tarefas…
Named Entity Recognition
Rotulagem de Papel Semântico
Part-of-Speech Tagging (Rotulagem de Classe Gramatical)
Bioinformática
O texto não estruturado tem uma natureza sequencial.

i.i.d : não é independente e identicamente distribuída

Named Entity Recognition
Identificar nomes de pessoas, locais etc no texto

people		organizations		places
Michael Dell is the CEO of Dell Computer Corporation and lives in Austin Texas.

Extrair partes de informação relevante para uma dada aplicação

make	model	year		mileage	price
For sale, 2002 Toyota Prius, 20,000 mi, $15K or best offer. 
Available starting July 30, 2006.

Rotulagem de Papel Semântico
Determina o papel semântico de cada noun phrase que é argumento do verbo

agent	patient	source	destination	instrument
John drove Mary from Austin to Dallas in his Toyota Prius.
The hammer broke the window.
Noun Phrase = Frase substantiva

Quem é o agente da ação?
Quem é o agente paciente?
Qual a Origem?
Qual o destino?
Qual é o instrumento da ação?

Bioinformática
Rotular sequências genéticas

exon			intron
AGCTAACGTTCGATACGGATTACAGCCT
Conjunto de sequências genéticas codificantes e não-codificantes do genoma humano
Exons: sequências codificantes
Introns: sequências não codificantes

Como identificar cada um desses rótulos?
Part-of-Speech Tagging
Named Entity Recognition 
São importantes para identificar a estrutura de uma sentença e o seu significado

Part-of-Speech -> Classes gramaticais das palavras

Named Entity Recognition -> Identificar quais são as palavras que representam entidades do mundo real

Part-of-speech Tagging
Atribuir a classe gramatical a cada palavra de uma sentença(substantivo, adjetivo, verbo etc)
Útil para tarefa de desambiguação: palavras podem ter mais de uma classe gramaticalEx: book, that etc
Classe mais frequente da palavra já tem alta acurácia
John saw the saw  and decided  to  take  it   to   the  table.
 PN     V    Det   N    Con     V          Part    V     Pro   Prep   Det     N
Desambiguação
Saw: “viu” (verbo)
Saw: “Serrote”  (substantivo comum)

Classe mais frequente -> alta acurácia -> o modelo irá predizê-la pois é mais frequente (maior a chance de acerto)

Part-of-speech Tagging
Textos em inglês
Pequena proporção das palavras possui mais de uma classe (85-86%)
Palavras com mais de uma classe gramatical são mais frequentes (55-67%)

WSJ (Wall Street Journal)

Part-of-speech Tagging

Closed class:
Preposições e pronomes
Tendem a ser curtos
Alta frequência
Open class:
Substantivos, verbos, adjetivos e advérbios
Constantemente sendo criados
17 principais Classes gramaticais em inglês -> Conjunto de Dependencias Universais

Closed: muito dificilmente aparecem preposições novas

Open: são criados novos constantemente


POS no Penn Treebank
45 classes detalhadas -> amplia o conjunto anterior

Exemplos de rotulagem
There/PRON/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS there/ADV/RB ./PUNC/.


Preliminary/ADJ/JJ findings/NOUN/NNS were/AUX/VBD reported/VERB/VBN in/ADP/IN today/NOUN/NN ’s/PART/POS New/PROPN/NNP England/PROPN/NNP Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP

Named Entity Recognition (Information Extraction)
Named entity: tudo que se refere a um nome próprio (regra geral)




Pode ser qualquer entidade: produto, doenças etc
Usado em Natural Language Understanding: Q&A, chatbot
Dificuldades:
Encontrar o pedaço do texto que contém a entidade
Ambiguidade: JFK (pessoa ou aeroporto)
JFK -> John F. Kennedy

Airport -> Nova York

BIO Tagging
Convenção para rotulagem de sequência
B - Beginning
I - Inside
E - End
O - Outside

Modelos de Rotulagem Sequencial
Hidden Markov Model (HMM)
Recurrent Neural Networks (RNN)
O Modelo deve atribuir um rótulo para cada palavra da sequência

Hidden Markov Model para POS Tagging
Modelo probabilístico sequencial
Computa a probabilidade para possíveis sequências de rótulos
Escolhe a melhor sequência
Rótulos estão escondidos (hidden)
Observa palavras
Inferir rótulos (ex. POS) da sequência de palavras
É um modelo clássico que introduz muitos dos conceitos chave para a modelagem sequencial que são usados por vários modelos atuais

Exemplo: dado que você só vê como as pessoas estão vestidas, tente prever como tá o clima no dia
Variável escondida: o clima
Variável observada: o tipo de roupa vestida



No caso de PLN

Dado um conjunto de palavras -> inferir as classes gramaticais

Variável escondida: os labels a serem atribuídos (Postag, NER)
Variavel observada: palavras


Markov Chain
Quando prevendo o rótulo futuro, o passado não importa. Somente o estado presente.
Markov Assumption
P(qi = a|q1...qi−1) = P(qi = a|qi−1)
π = [0.1, 0.7, 0.2]
Para prever o clima de amanhã você vai se basear apenas no clima de hoje

Pi: distribuição de probabilidade inicial

A soma dos pesos das transições saindo de um determinado estado deve ser igual a 1

ESTADOS
OBSERVAÇÕES
Hidden Markov Model: Componentes
q1
q2
q3
o1
o2
o3
a12
a23
b1(o1)
b2(o2)
b3(o3)
Probabilidade de mover do estado i ao estado j
=
Matriz de Probabilidade de Transição
Probabilidade da Observação oi ter sido gerada pelo estado qi
Markov Assumption: P(qi = a|q1...qi−1) = P(qi = a|qi−1)
Output Independence: P(oi|q1, . . ., qi, . . . , qT , o1, . . . , oi, . . . , oT ) = P(oi|qi)
Matriz A
Matriz B
Observações: Palavras
Estados: classes gramaticais

Matriz A (matriz de transição): as probabilidades de uma classe vir após outra
Matriz B: a probabilidade de uma classe gramatical ser atribuída a uma palavra observada

HMM Tagger: Componentes
Matriz A: probabilidades de transição das tags
Exemplo No corpus WSJ, MD ocorre 13.124 no corpus sendo que 10.471 vezes a tag MD aparece seguida pela tag VB. Logo, a probabilidade de termos uma tag MD seguida de VB é:
A probabilidade de uma tag ocorrer observando-se uma tag anterior
Contagem
MD: ModalVB: Verb

HMM Tagger: Componentes
Matriz B: probabilidades de uma palavra associada a uma tag
Exemplo Das 13.124 ocorrências da tag MD no corpus WSJ, a tag está associada 4046 vezes à palavra “will”. Logo, a probabilidade de termos a palavra “will” associada à tag MD é dada por:
A probabilidade de uma palavra estar associada a uma determinada tag
Contagem

HMM Tagger: Exemplo
Os nós são os estados escondidos que estamos tentando prever.

As arestas são aquelas porcentagens de transição entre os estados escondidos

Os quadros são as matrizes de pesos contendo as probabilidades de a classe pertencer a uma determinada palavra

Com esse modelo probabilístico será possível fazer as predições para uma determinada sentença

HMM Decoding
Dadas as matrizes A e B como entrada, assim como a sequência de palavras (observações), o objetivo é encontrar a sequência de tags mais prováveis.
Teorema de Bayes
Suposições:
A probabilidade de uma palavra aparecer na sequência é independente da vizinhança e depende somente da sua tag;
A probabilidade de uma tag depende somente da tag anterior
Matriz A
Matriz B
Dado o modelo probabilístico é uma sequência de palavras o objetivo é encontrar a sequência de tags (classes gramaticais) mais prováveis para aquele texto

Maximizar a probabilidade -> gerar a melhor sequência de tags (rótulos) t1

HMM: O Algoritmo Viterbi
Matriz A
Matriz B
Valores computados do corpus Wall Street Journal (WSJ)
Entrada: Janet will back the bill
É o algoritmo que vai fazer a decodificação do HMM para rotulagem de uma sentença dada como entrada

Matriz A e B -> baseada no WSJ (Wall Street Journal) corpus

JJ: Adjetivo
VB: Verbo
NNP: Substantivo Proprio
MD: Modal
DT: Determinante
RB: Adverbio
NN: Substantivo comum

Programação Dinâmica

HMM:
O Algoritmo Viterbi
Distribuição de probabilidade inicial. Na Matriz A é dado por <s>
O Algoritmo começa pela distribuição de probabilidade inicial, essa rotulagem guiará a predição das próximas tags

Após isso será desenvolvido o cálculo de forma a maximizar a probabilidade final da sentença inteira

1 - A probabilidade um substantivo próprio (NNP) iniciar a sentença
2 - A probabilidade de um substantivo próprio (NNP) ser Janet
3 - Multiplica as duas probabilidade

Faz isso para todas as tags. Seleciona a maior probabilidade

BACKTRACE = Registra os estados anteriores para quando chegar ao final da sequência, saber qual o caminho correto percorrido.

Sequencia final: NNP MD VB DT NN

Células em Branco = São 0.

“the” e “bill = a completar

HMM: O Algoritmo Viterbi
O valor de cada célula é computado recursivamente obtendo o caminho mais provável
Sentença de entrada: janet will back the bill

HMM
É um modelo generativo: 
modela como os dados foram gerados e depois aplica o que foi aprendido para classificar cada item da sequência
Modela a distribuição de probabilidade conjunta
Modelo útil e poderoso
Problemas:
Precisa de muitos dados para alcançar boa acurácia
Dificuldade nas tarefas NLP: a existência de palavras desconhecidas
Nomes próprios ou acrônimos por exemplo
Limitação das features:
Não diferencia maiúsculas ou minúsculas
Não considera o contexto anterior da palavra

Recurrent Neural Networks (RNN)

Extração de Relações
Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it has increased fares by [MONEY $6] per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as [LOC Chicago] to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].

Extração de Relações
Já existem alguns padrões para detecção de relações

PER = Pessoa
GPE = Entidade Geo-Política
ORG = Organização

Extração de Relações Baseada em Padrões
Padrões léxicos-sintáticos
Hearst Patterns para extração de hipônimos
“Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.”
∀NPi, i ≥ 1, hyponym(NPi, NP0)
hyponym(Gelidium, red algae)
NP0 such as NP1{, NP2 . . . , (and|or)NPi}, i ≥ 1
são palavras de sentido específico, ou seja, palavras cujos significados são hierarquicamente mais específicos do que de outras
Heart Patterns -> Hipônimos

NP: Noun Phrase -> Frase Substantiva  + “such as”

NP0: red algae
NP1: Gelidium

NP1 é alga vermelha é hipônimo de Gelidium

Gelidium é um hiperônimo de Alga Vermelha

Extração de Relações Baseada em Padrões
Hiperônimos são palavras de sentido genérico, ou seja, palavras cujos significados são mais abrangentes do que os hipônimos:

Animais é hiperônimo de cachorro e cavalo.
Legume é hiperônimo de batata e cenoura.
Galáxia é hiperônimo de estrelas e planetas.
Templos (hiponomio) = civic building (hipernomio)

Herrick, GOldsmith, Shakespeare = Autores

Canada, England = Common-law countries

Extração de Relação Baseada em ML
Definem-se as relações e entidades a serem extraídas
Anotam-se exemplos para treinamento
Detecção de entidades seguida da identificação de relações
Features:
BOW e bigramas nas entidades
American, Airlines, Tim, Wagner, American Airlines, Tim Wagner
Palavras ou bigramas ao redor
Tipos das entidades
Número de entidades entre as entidades candidatas
Part-of-speech

Extração de Relação Baseada em Redes Neurais
Dado que a sentença possui um Substantivo próprio e um objeto relativo a localização
O classificador obterá a probabilidade de a sentença conter uma relação de nascimento
Entre essa pessoa e a cidade mencionados

Relação = local de nascimento

Aula Prática
Extração de Informação com RNN

Exercício Proposto
Treine uma Rede Neural, agora a nível de tokens, para detecção de entidades nomeadas no conjunto de dados Conll 2002

Referências
Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/8.pdf>. Capítulo 8. Acesso em: 03 de Março de 2023.

Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/21.pdf>. Capítulo 21. Acesso em: 03 Março de 2023.

// File: content/slides/Aula 5 - Geração Automática de Texto.txt

Processamento de Linguagem Natural
Geração Automática de Texto

Geração Automática de Texto
Natural Language Generation (NLG)
Tarefa de produzir texto coerente a partir de dados estruturados ou não estruturados
Utilizados para diversas tarefas

Aplicações
FONTE: http://demo.clab.cs.cmu.edu/NLP/S21/files/slides/23-NLP_Generation.pdf 

Aplicações
FONTE: http://demo.clab.cs.cmu.edu/NLP/S21/files/slides/23-NLP_Generation.pdf 

Aplicações
FONTE: http://demo.clab.cs.cmu.edu/NLP/S21/files/slides/23-NLP_Generation.pdf 

Modelos Neurais de NLG
Visão Geral

Modelo Auto-Regressivo
FONTE: http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture12-generation.pdf 

Modelo Auto-Regressivo
FONTE: http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture12-generation.pdf 

Modelo Auto-Regressivo
FONTE: http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture12-generation.pdf 

Decoding: Greedy Search
FONTE: https://huggingface.co/blog/how-to-generate
Seleciona a próxima palavra com base na probabilidade máxima
Perda de diversificação no texto gerado

Decoding: Beam Search
FONTE: https://huggingface.co/blog/how-to-generate
Mantém um num_beams de hipóteses que nos passos futuros podem apresentar probabilidade máxima

Decoding: Beam Search
Pode gerar repetição ou textos “monótonos”
Problema geral em geração de texto
FONTE: https://arxiv.org/pdf/1904.09751.pdf
Falta de diversificação na geração do texto

Humanos vs Beam Search
Linguagem humana de alta qualidade não segue uma distribuição máxima de palavras.
É preciso que o texto surpreenda, não seja previsível
FONTE: https://arxiv.org/pdf/1904.09751.pdf

Decoding: Random Sampling
wt​∼P(w∣w1:t−1​)
Distribuição de probabilidade condicional
O modelo de linguagem passa a ser não-determinístico
FONTE: https://huggingface.co/blog/how-to-generate
A randomicidade não pode ser feita de qualquer forma. Pois, pode apresentar problemas do tipo:
-> Texto desconexo
-> Texto sem sentido
-> Geração totalmente aleatória


Decoding: Random Sampling

wt​∼P(w∣w1:t−1​)
Distribuição de probabilidade condicional
Pode gerar texto incoerente
FONTE: https://huggingface.co/blog/how-to-generate

Softmax Temperature
Regula a diversidade da saída: τ



Valores de τ > 1: Pt mais distribuída pelo vocabulário (saída mais diversa)
Valores de τ < 1: Pt mais concentrada nas palavras com maior probabilidade (saída menos diversa) 
τ→0 = greedy decoding
Resolvendo a previsibilidade.
Deixa a seleção da próxima palavra menos randômica.
Diminuir a temperatura, aumenta a coerência

Softmax Temperature
FONTE: https://huggingface.co/blog/how-to-generate
τ = 1
τ = 0.7

Decoding: Top-K Sampling

FONTE: https://huggingface.co/blog/how-to-generate
Filtra as k palavras mais prováveis
A distribuição de probabilidade é redistribuída entre essas palavras


Decoding: Top-p (Nucleus) Sampling

FONTE: https://huggingface.co/blog/how-to-generate
Escolhe entre o menor conjunto de palavras cuja probabilidade cumulativa ultrapassa o valor de p
A distribuição de probabilidade é redistribuída
Ex.: p=0.9

Qual o menor número de palavras cuja probabilidade cumulativa é maior que “p”?
p=0.9

Decoding
Top-p and top-K em geral produzem texto de melhor qualidade que Greedy e Beam Search
Modelos permitem utilizar uma combinação dessas abordagens

NLG: Avaliação de Modelos

NLG: Avaliação de Modelos por sobreposição
Rápido, eficiente e bastante utilizado
Pontuação indicando a similaridade entre o texto gerado e o esperado (escrito por humano)
Métricas baseadas em overlaps de n-grams
BLEU, ROUGE, METEOR, CIDEr, etc.
Métricas baseadas em semântica
PYRAMID, SPICE, SPIDEr, etc.

Avaliação
ROUGE
Sumarização Textual
Compara um sumário a um ou mais sumários de referência
Orientada a revocação
BLEU Score
Tradução
Compara a tradução gerada com referências escritas por humanos
Orientada a precisão
João ama tomar café.
unigrama
bigrama
n-grama

Desafios: avaliação dos modelos “Tunados”
João ama tomar café.
João adora beber café.
ROUGE-1
# matches de unigramas
# unigramas na referência
=
Referência
Gerado
ROUGE-1
# matches de unigramas
# unigramas na saída gerada
=
=
=
2
4
2
4
Precision
Recall
ROUGE-1
ROUGE-2
…
ROUGE-L
Subsequência comum mais longa
ROUGE
Sumarização Textual
Compara um sumário a um ou mais sumários de referência
Orientada a revocação
=
0.5
=
0.5
https://aclanthology.org/W04-1013/ 

Avaliação: ROUGE

João ama tomar café.
ROUGE-1
# matches de unigramas
# unigramas na referência
=
Referência
Gerado
ROUGE-1
# matches de unigramas
# unigramas na saída gerada
=
=
=
4
4
Precision
Recall
ROUGE-1
ROUGE-2
…
ROUGE-L
Subsequência comum mais longa
Tomar João café ama.
ROUGE
Sumarização Textual
Compara um sumário a um ou mais sumários de referência
Orientada a revocação
=
1.0
4
4
=
1.0
https://aclanthology.org/W04-1013/ 

Avaliação: BLEU Score
Tradução
Compara a tradução gerada com referências escritas por humanos
Orientada a precisão
Penalidade de Brevidade: penaliza as gerações curtas
BLEU = min( 1, 
tamanho da geração
tamanho da referência
)(П precisioni)1/4
4
i=1
overlap de n-gramas
penalidade de brevidade
https://aclanthology.org/P02-1040.pdf 

Avaliação: BERTScore
FONTES:
https://huggingface.co/spaces/evaluate-metric/bertscore 
https://arxiv.org/pdf/1904.09675.pdf 
https://github.com/Tiiiger/bert_score 

Como explorar Modelos de Linguagem?
Prompt Engineering
Fine-tuning

Translate English to Portuguese:
house =>
Translate English to Portuguese:
house => casa
O que é um prompt?
É o texto dado como entrada ao modelo
LLM
PROMPT
SAÍDA
INFERÊNCIA
Janela de Contexto
* Resultado obtido com o ChatGPT

Translate English to Portuguese:
house => casa
O que é um prompt?
É o texto dado como entrada ao modelo
LLM
PROMPT
SAÍDA
INFERÊNCIA
In-Context Learning: Zero-shot
Translate English to Portuguese:
house =>
* Resultado obtido com o ChatGPT

Translate English to Portuguese:
house => casa
bakery => padaria
bread => pão
coffee => café
Translate English to Portuguese:
house => casa
bakery => padaria
bread => pão
coffee => 
O que é um prompt?
LLM
PROMPT
SAÍDA
INFERÊNCIA
In-Context Learning: Few-shot
* Resultado obtido com o ChatGPT

Tamanho da Janela de Contexto
FONTE:
[1] https://www.cerebras.net/blog/variable-sequence-length-training-for-long-context-large-language-models/
[2] https://www.mosaicml.com/blog/mpt-7b  
[3]https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/ 
[4] https://developers.generativeai.google/models/language 
MPT-7B-StoryWriter-65k+
* Figura original obtida de [1] e alterada para inclusão do modelo open-source MPT-7B-StoryWriter-65k+ [2]
closed models
OPEN MODELS

[3] Cerebras-GPT
Abril (2023)
Sequence Length: 2048

[4] PALM 2
Maio (2023)
Sequence Length: 8K

Prompt Engineering: Limitações
Tamanho da janela de contexto é limitado
Requer esforço humano para construção e melhoria de prompts
Um mesmo prompt pode entregar saídas diferentes
Inconsistência na experiência com usuário
Melhores saídas geralmente são obtidas com o uso repetitivo de prompts, em um formato conversacional 

Por que utilizar modelos Open-Source?
Fine-tuning: Não precisar treinar do zero ou depender apenas de prompt engineering
OpenAI disponibiliza fine-tuning apenas para alguns modelos através de sua API
Não depender de terceiros
Perda de disponibilidade do serviço
Alteração nos custos do serviço
Latência
Moderação e Segurança: dados sendo transmitidos para terceiros
Controle total sobre qualquer operação
Mais customizáveis
Privados
Existem modelos abertos com menor número de parâmetros mas com performance competitiva
LLaMA (65B) apresenta maior performance que GPT-3 (175B) e PaLM (540B) [1]
Chinchilla (70B) apresenta maior performance que GPT-3 (175B) and Gopher (280B) [2]
[1] https://arxiv.org/abs/2302.13971
[2] https://arxiv.org/pdf/2203.15556.pdf 

Limitação de LLMs Pré-treinados

Generalistas e não respondem bem a instruções
LLM Pré-treinado

Supervised Fine-tuning
Treina o LLM com instruções variadas ou focadas em uma única tarefa
https://vitalflux.com/instruction-fine-tuning-llm-explained-with-examples/

Exemplos de Instruções
Fonte: https://huyenchip.com/2023/05/02/rlhf.html
https://cameronrwolfe.substack.com/p/understanding-and-using-supervised

Resultado do Fine-tuning
https://vitalflux.com/instruction-fine-tuning-llm-explained-with-examples/

Exemplo de Saídas de Modelos sem e com Fine-tuning
LLMs Pré-treinados (sem fine-tuning)
LLMs após Fine-tuning
Tarefa: responder comentários ruins sobre restaurantes

Fine-tuning
É possível obter bons resultados a partir de um pequeno conjunto de dados
Full fine-tuning demanda bastante recursos computacionais

Fine-tuning: Diminuindo os custos computacionais e de armazenamento
Técnicas para redução do número de pesos “tunáveis” dos modelos
Parameter-Efficient Fine-Tuning (PEFT)
Quantização: Técnica para “tunagem” de parâmetros com menor precisão numérica
Apesar dos esforços, treino e inferência ainda demandam bastantes recursos computacionais

https://github.com/huggingface/peft 

Parameter Efficient Fine-Tuning (PEFT)
Permite a adaptação do fine-tuning de um LLM pré-treinado
Congelam pesos ou camadas do modelo
Atualizam apenas alguns pesos ou camadas
Adicionam camadas ou parâmetros
Diminuem os custos computacionais e de armazenamento
Performance comparável ao fine-tuning completo do LLM
Menos propenso a esquecimento catastrófico durante fine-tuning completo
FONTE: https://arxiv.org/pdf/2303.15647v1.pdf 

LLM pré-treinado
Low-Rank Adaptation (LoRA)
Entrada X
Pesos pré-treinados
W
Embedding h
Congela os pesos originais do modelo
FONTE: https://arxiv.org/abs/2106.09685 

LLM pré-treinado
Low-Rank Adaptation (LoRA)
Entrada X
Pesos pré-treinados
W
Embedding h
WA
WB
Congela os pesos originais do modelo
Injeta duas matrizes menores de decomposição que multiplicadas aproximam W

FONTE: https://arxiv.org/abs/2106.09685 

LLM pré-treinado
Low-Rank Adaptation (LoRA)
Entrada X
Pesos pré-treinados
W
Embedding h
WA
WB
Congela os pesos originais do modelo
Injeta duas matrizes menores de decomposição que multiplicadas aproximam W
r
=
X
WB
WA
W
A, r
r, B
FONTE: https://arxiv.org/abs/2106.09685 
rank
default=8

LLM pré-treinado
Low-Rank Adaptation (LoRA)
Entrada X
Pesos pré-treinados
W
Embedding h
WA
WB
+
Congela os pesos originais do modelo
Injeta duas matrizes menores de decomposição que multiplicadas aproximam W 

r
=
X
WB
WA
W
A, r
r, B
Treina os pesos das matrizes de decomposição
Adiciona os pesos aprendidos aos congelados
W
WA X WB
+
FONTE: https://arxiv.org/abs/2106.09685 
rank
default=8

Low-Rank Adaptation (LoRA)
Reduz o número de parâmetros treináveis
Possibilita a execução do fine-tuning com menos recursos computacionais
Possibilita Inferência mais customizada:
Adicionando as matrizes de decomposição, aprendidas anteriormente, aos pesos congelados do modelo em tempo de inferência
FONTE: https://arxiv.org/abs/2106.09685 

Quantização
PRECISÃO
MEMORY
EXEMPLO
FP32
4 bytes
3.1415920257568359375
FP16
2 bytes
3.140625
INT8
1 Byte
3
Técnica que reduz a quantidade de memória necessária para armazenar e treinar modelos
Projeta os valores originais com precisão de 32 bits em espaços de precisão menor.
Quantização + LoRA = QLoRA
https://huggingface.co/docs/transformers/perf_train_gpu_one#mixed-precision-training 
https://huggingface.co/docs/transformers/model_memory_anatomy 
https://cloud.google.com/tpu/docs/bfloat16 
QLoRA: https://arxiv.org/abs/2305.14314 
2GB @ 16-bit
half precision
4GB @ 32-bit
full precision
1GB @ 8-bit
precision
Armazenar 1B parâmetros

Desafios: Alucinação
Fonte: https://www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawsuit-avianca-airlines-chatbot-research

Desafios: Informação Desatualizada

Retrieval Augmented Generation (RAG)
Adição de conteúdo como entrada para a geração de texto
Consulta
Saída gerada entregue ao usuário
Busca
Documentos relevantes
+
Prompt aumentado com contexto adicional
Qual a proporção de candidatos negros ou pardos …  
50% dos ... 
LLM
Base de dados 
do domínio

Resultado da Adição de Conteúdo para 
Geração do Texto 

Desafios: Deployment de Aplicações baseadas em LLMs
Treino e Inferência
Treino do zero:
Muitos dados
Requer muita memória
Demanda poder computacional
Inferência com CPU: Técnicas de conversão para C++ [1]
4GB @ 32-bit
full precision
80GB @ 32-bit
full precision
Armazenar
Treinar
LLM
1B
[1] 
https://github.com/ggerganov/llama.cpp
https://pub.towardsai.net/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b 

Treino e Inferência
Treino do zero:
Muitos dados
Requer muita memória
Demanda poder computacional
Inferência com CPU: Técnicas de conversão para C++ [1]
Modelos generalistas pré-treinados
Formato de Saída ambíguo
Experiência do Usuário se torna inconsistente
Engenharia de Prompt
LLM
pré-treinado
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
Prompt
Input
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkkauhsiauhauishuiauhsiaushiauhsiuahsiuahsiuahsiuhaiu
Generated Text
Output
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
Generated Text
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkkshshaiuhsiaushiauhsiauhsiauhsiuahsiuhaiushaiuhsuiahsiuahshausiuahsiuahsuihaiushaiuhsuiahsuai
Generated Text
[1] 
https://github.com/ggerganov/llama.cpp
https://pub.towardsai.net/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b 
Desafios: Deployment de Aplicações baseadas em LLMs

Treino e Inferência
Treino do zero:
Muitos dados
Requer muita memória
Demanda poder computacional
Inferência com CPU: Técnicas de conversão para C++ [1]
Modelos generalistas pré-treinados
Formato de Saída ambíguo
Experiência do Usuário se torna inconsistente
Engenharia de Prompt
Escalabilidade
LLM
pré-treinado
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
Prompt
Input
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
asakjshkjahskjhkjahsjkhkajshkajshkjhskjahskjahskjahskjakjshjkahskjahkjshakjhskjahskjahkjsjakskjahskjahskjahskjhakjshkjahskjahsjkk
…
[1] 
https://github.com/ggerganov/llama.cpp
https://pub.towardsai.net/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b 
Desafios: Deployment de Aplicações baseadas em LLMs

Ética em Modelos de NLG: Tay Chatbot
Chatbot criado pela MS em 2016
Em 24 horas, começou a fazer comentários racistas, sexistas, seguindo estereótipos negativos e aprendendo padrões nocivos
Criados a partir do language model (viés no corpus)
Sheng et al., EMNLP 2019

Ética em Modelos de NLG
Viés de Gênero

Ética em Modelos de NLG
Viés de Gênero
Privacidade e Anonimato

Ética em Modelos de NLG
Viés de Gênero
Privacidade e Anonimato
GPT-2 Release. FONTE: https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction 

Ética em Modelos de NLG
Viés de Gênero
Privacidade e Anonimato
Criação de Notícias Falsas
GPT-2 Release. FONTE: https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction 

https://www.technologyreview.com/2023/02/14/1068498/why-you-shouldnt-trust-ai-search-engines
Ética em Modelos de NLG

https://time.com/6247678/openai-chatgpt-kenya-workers/
Ética em Modelos de NLG

Referências
Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/7.pdf>. Capítulo 6. Acesso em: 01 Setembro de 2022.
Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/11.pdf>. Capítulo 11. Acesso em: 01 Setembro de 2022.
https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
Tokenizers: How machines read. Disponível em: <https://blog.floydhub.com/tokenization-nlp/>. Acesso em: 01 Setembro de 2022. 
Data Science Academy. Deep Learning Book, 2022. Disponível em: <https://www.deeplearningbook.com.br/>. Capítulos: 76 a 82. Acesso em: 01 Setembro. 2022.
Data Science Academy. Deep Learning Book, 2022. Disponível em: <https://www.deeplearningbook.com.br/>. Capítulos: 86 a 90. Acesso em: 01 Setembro. 2022.

Aula Prática
Google Colab Text Generation com Transformers


// File: content/slides/Aula 6 - Recuperação de Informação.txt

Processamento de Linguagem Natural
Recuperação de Informação

Q&A baseado em Recuperação de Informação

Aplicações

Aplicações

Aplicações

IBM Watson

Modelo de Espaço de Vetores
Documento e consulta representados por um vetor de palavras
Cada palavra é uma dimensão do vetor



Modelo de Espaço de Vetores
Espaço é do tamanho do vocabulário (alta dimensão)
Documentos são vetores esparsos
Similaridade entre os vetores da consulta e dos documentos


Similaridade de Jaccard
Tamanho da intersecção dividido pela união













Exemplo: JSim(C1,C2) = 3/8

Similaridade de Cosseno
Documentos ranqueados pela proximidade de pontos representando a consulta e os documentos


Cálculo da Similaridade
Considere dois documentos D1 e D2 e uma consulta Q 
D1 = (0.5, 0.8, 0.3), D2 = (0.9, 0.4, 0.2), Q = (1.5, 1.0, 0)

Modelo de Espaço de Vetores
Vantagens:
Eficiente
Permite casamento parcial
Fácil de implementar
Funciona bem na prática
Cons:
Assume independência dos termos
Sem informação semântica e sintática

Peso dos Termos
Termos em um documento não são igualmente úteis para descrever seu conteúdo
Ex: palavras frequentes no documento -> importantes
Ex: palavras que aparecem em todos documentos da coleção -> não importantes
Peso usado para caracterizar a importância do termo
Útil para computar ranqueamento de documentos dada uma consulta
Documentos com termos da consulta com alto peso são melhores ranqueados


Frequência do Termo no Documento - TF	
Intuição: a importância do termo em um documento é proporcional à sua frequência nele










Frequência do Termo	

Inverse Document Frequency (IDF)
Medir a especificidade de um termo
Não mede a especificidade semântica de um termo 
Depende do seu significado
Pode ser usado um thesaurus: wordnet
Ex: o termo bebida é mais genérico que café ou chá
Em RI, especificidade estatística ao invés da semântica
O inverso do número de documentos nos quais o termo ocorre



Usado amplamente em algoritmos de ranqueamento

Inverse Document Frequency (IDF): Variações

Inverse Document Frequency (IDF)

TF-IDF
Combinação do tf com o idf


Variações:



TF-IDF
TF
IDF

TF-IDF
TF
IDF

TF-IDF
TF
IDF

Exemplo: Cálculo do Cosseno usando TF-IDF
Consulta: to do

BM25
Um dos mais populares e efetivos algoritmos de ranqueamento
Baseado no BIM
3 princípios básicos: tf, idf e normalização pelo tamanho do documento
Criado como resultado de experimentos em variações de modelos probabilísticos
Usado como baseline em experimentos de RI


BM25






k1, k2 e b são parâmetros definidos empiricamente (depende da coleção)


dl: tamanho do documento

BIM
TF
Normalização
pelo tamanho
do documento 

BM25: Exemplo

BM25: Exemplo

Busca Semântica
Limitação de abordagens léxicas: 
Assume interseção entre vocabulário da pergunta e resposta
Comparação busca léxica vs semântica

Bi-encoder vs Cross Encoder
Bi-encoder
Dois encoders independentes
Eficiente: representações podem ser pré-computadas
Cross Encoder
Único encoder
Captura interações entre o par de entrada
Maior custo computacional

Retrieval Augmented Generation
Fonte: https://medium.com/enterprise-rag/an-introduction-to-rag-and-simple-complex-rag-9c3aa9bd017b

Métricas de Avaliação

Métricas
Precisão: fração de docs relevantes do total recuperados


Revocação: fração dos docs relevantes recuperados

Precisão e Revocação
Todos os docs da coleção avaliados
Em engenhos de busca: 
Docs não apresentados de uma única vez
Lista ranqueada dos docs
Varia com a posição do ranqueamento
Mais apropriado: curva de precisão e revocação

Precisão@k
Independente de recall
Número de docs relevantes no topo do ranqueamento
Precisão  em 5 (P@5): mede a precisão qdo 5 docs são apresentados

Exemplo
Resultado de consulta:






P@5= 40% e P@10= 40%

Mean Reciprocal Rank
Medir a primeira resposta correta
Q&A
Busca por sites ou URLs

MRR: Mean Reciprocal Rank
Reciprocal ranking
Ri: ranqueamento relativo à consulta qi
Scorrect(Ri): posição da primeira resposta correta em Ri
Sh: limiar para posição no ranqueamento




Para um conjunto de consultas

E-Measure
Combina precisão e revocação



b: parâmetro para dar mais peso para precisão ou revocação
(definido pelo usuário)
r(j): revocação na posição j do ranqueamento
P(j): precisão  na posição j do ranqueamento

E-Measure



b=0 
E(j) = 1 - P(j) 
E(j) se torna uma função de precisão
b -> ∞
limb -> ∞ E(j) = 1 - r(j)
E(j) se torna uma função de revocação
b=1: F-measure (média harmônica)

F-Measure
Valor único que combina precisão e revocação




Assume valores entre 0 e 1
F(j)=0: nenhum documento relevante é recuperado
F(j)=1: todos documentos relevantes são recuperados
Assume valor alto apenas quando ambos precisão e revocação são altos
Para maximizar F-measure, é preciso encontrar o melhor compromisso entre precisão e revocação

Correlação de Ranqueamento
Em alguns casos
Não conseguimos avaliar relevância diretamente
Interessados em determinar quanto um ranqueamento diferencia de outro já conhecido
Comparar ordem relativa de dois ranqueamentos
Técnicas estatísticas de correlação de ranqueamento

Correlação de Spearman
R1e R2: ranqueamentos a serem comparados
K: número de elementos no ranqueamento
sij: posição do documento no ranqueamento i na posição j 
Métricas
Correlação=1: ranqueamentos iguais
Correlação=0: completamente independentes
Correlação=-1: ranqueamento inverso


Correlação de Spearman

Correlação de Spearman

Kendal Tau
Correlação de Spearman não tem uma interpretação clara
Kendal Tau: baseado na ideia de pares de docs concordantes e discordantes em dois ranqueamentos

Kendal Tau
Considere os ranqueamentos dados por R1 e R2:





Ranqueamentos de docs por R1 e R2:

Pares de docs ordenados de R1
Pares de docs ordenados de R2

Kendal Tau
Contar or pares concordantes e descordantes
No exemplo anterior:
Para um total de 20 pares ordenados: K(K-1)
14 concordantes e 6 discordantes
Coeficiente de Kendal Tau


No exemplo:



Kendal Tau
Seja:
Número de pares discordantes:
Total de pares:  

Coeficiente kendal tau



No exemplo anterior:

Aula Prática
RAG com LlamaIndex


// File: content/slides/Aula 7 - Chatbots.txt

Processamento de Linguagem Natural
Chatbots
Prof. Luciano Barbosa

O que são
Sistemas interativos
Uso de linguagem natural
Entrada: texto ou fala
Saída: texto ou fala
IMG Source: http://ai.stanford.edu/blog/chirpy-cardinal/

Histórico
Confunde-se com a história de IA
1950-60: Turing e Weizenbaum
Computadores se comunicando como humanos
1966: Eliza (Weizenbaum)
1980-2000: projetos da DARPA
Comunicação automática com soldados
Serviço de FAQ
Teste de Turing: “se um ser humano conversa com uma máquina por cinco minutos sem perceber que ela não é humana, o computador passa no teste”

Eliza: primeiro sistema conversacional

DARPA: Defense Advanced Research Projects Agency

Exemplos
Google Assistant
Siri
Alexa
Cortana
Ex: Chatbots em sites de e-commerce ou para atendimento ao cliente

Dois tipos de Agentes Conversacionais
Agentes baseados em objetivo:
Conversas curtas
Realização de uma tarefa
Ex: reserva de hotéis ou restaurantes
Bate-papo:
Conversas mais longas
Mais próximo à interação humana (mais natural)
Ex: ELIZA
Baseados em objetivo: São agentes com função clara e específica

Bate-papo: chit·chat

Propriedades da Conversação Humana
Turns, Atos da Fala e Grounding
Turns: o agente deve identificar a vez de falar
Atos da Fala:
Grounding: Garantir que a mensagem foi recebida/entendida


Fonte: Jurafsky e Martin (2021)
Agente de viagem

C: Cliente
A: Agente

Turns
Garantia do recebimento da mensagem
Saber quando a pessoa terminou de falar
Saber quando o cliente muda de ideia no meio da conversação
Ruídos na conversação

Turns:
Cada indivíduo tem a palavra de tempos em tempos
Diálogo é uma sequência de turns
Sistema precisa saber quando falar (ex.: não esperar muito tempo depois que a pessoa terminou de falar)
Sistema precisa saber quando a pessoa terminou de falar (desafios: ruído e pausas no meio da fala)
Propriedades da Conversação Humana

Atos da Fala
Constativa: comprometer o orador com alguma condição
Responder, confirmar, negar, não concordar
Diretiva: tentativa do orador de ser atendido
Aconselhar, perguntar, pedir, convidar
Comissiva: comprometer o orador com algo futuro
Prometer, planejar, apostar, se opor
Reconhecimento: expressar reconhecimento sobre alguma atitude
Desculpar, agradecer, cumprimentar, aceitar, reconhecer
Cada sentença da conversação é algum tipo de ação executada por quem está falando (seja o usuário ou o agente)

Diretiva: 
“toque música”

S: “Que dia você quer viajar?”

Constativa:
U: “vou viajar em maio!”

É um importante componente para detectar a Intenção do usuário


Exemplo
Fonte: Jurafsky e Martin (2021)
Diretiva
Constativa
Comissiva
Reconhecimento
Diretiva: Aconselhar, perguntar, pedir, convidar

Constativa:  Responder, confirmar, negar, não concordar

Comissiva: Prometer, planejar, comprometer o orador com algo futuro

Reconhecimento: agradecer, cumprimentar, aceitar, reconhecer

Grounding
Indicar que o foi dito, foi compreendido:
A: And you said returning on May 15th?
C: Uh, yeah, at the end of the day.
A: OK

C: OK I’ll take the 5ish flight on the night before on the 11th.
A: On the 11th? OK.

C: ...I need to travel in May.
A: And, what day in May did you want to travel?
A pessoa receber alguma indicação/feedback/retorno de que a mensagem foi recebida e será respondida/resolvida de acordo

Sequências Secundárias
Sub-diálogos, Pré-sequências & Iniciativa
Um conversa tem estrutura

Perguntas geram a necessidade de uma resposta
Uma Proposta, ela gera a necessidade de uma Confirmação ou Rejeição da proposta


Sub-Diálogos: Correção
C17: #Act. . . actually#, what day of the week is the 15th?
A18: It’s a Friday.
C19: Uh hmm. I would consider staying there an extra day til Sunday.
A20: OK...OK. On Sunday I have ...
O cliente interrompe a conversa principal
O agente agora precisa responder o novo questionamento
O agente precisa detectar que o cliente agora está considerando voltar no dia 17 e não mais no dia 15.
Ainda o agente de viagem…

Um exemplo de correção

Sub-Diálogos: Clarificação
User: What do you have going to UNKNOWN_WORD on the 5th?
System: Let’s see, going where on the 5th?
User: Going to Hong Kong.
System: OK, here are some flights...
Ajuda o agente a confirmar a falta de entendimento da entrada anterior

Pré-sequências
User: Can you make train reservations?
System: Yes I can.
User: Great, I’d like to reserve a seat on the 4pm train to New York.
Identifica Intenções
Exemplos: em casos de múltiplas intenções
O usuário já iniciou a conversação perguntando se o sistema poderia fazer algo.

Reserva de trem

Intenções diferentes: reserva trem, reserva voo, reserva ônibus

Iniciativa do Diálogo
Controlada por uma pessoa
Ex: Entrevista
Mixed
Conversa natural entre pessoas
Difícil para sistemas de PLN
Iniciativa do usuário: pergunta ou fala comandos
Iniciativa do sistema: sistema faz perguntas para preencher formulário

Agentes Conversacionais
Chatbots
Para bate-papo

Chatbot: Bate-Papo
Inicialmente utilizado na área de psicologia
Baseado em regras
Eliza, Parry

Baseado em corpus (corpus de conversações)
Recuperação de Informação
Neural network encoder-decoder
Ex: BlenderBot, Xiaolce (Little Bing)
Conversa entre um psicólogo e seus pacientes

Conversa: utterance

BlenderBot

Chatbots
Baseados em Regras

Chatbots baseados em regras
Clássico:
Eliza (1966)
Parry (1968):
Usado para estudar esquizofrenia
Primeiro sistema a passar no teste de Turing
Estados de medo e raiva
Psiquiatras não conseguiram diferenciar transcripts de entrevistas reais com as produzidas por ele
Eliza: psicologia

Parry: psiquiatria

Eliza: Weizenbaum (1966)
Men are all alike.
IN WHAT WAY?
They're always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE?
Well, my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE?
He says I'm depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
...
WHO ELSE IN YOUR FAMILY TAKES CARE 0F YOU?
My father
YOUR FATHER
You are like my father in some ways.
WHAT RESEMBLANCE DO YOU SEE?
You are not very aggressive but I think you don’t want me to notice that.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE?
You don’t argue with me.
WHY DO YOU THINK I DON’T ARGUE WITH YOU?
You are afraid of me.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU?
My father is afraid of everybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER?
Bullies.
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HERE?
Preto: paciente
Verde: o chatbot

Como funciona?
Rogerian psychologist
Estimula o paciente a falar refletindo suas frases de volta a ele
Não conhece quase nada sobre o mundo real
Objetivo é de continuar o diálogo
Patient: "I went for a long boat ride”
Psychiatrist: "Tell me about boats”
Foi projetado para simular um psicólogo rogeriano

Exemplo de Regra
(0 YOU 0 ME) →(WHAT MAKES YOU THINK I 3 YOU) 
User: You hate me
Agent: WHAT MAKES YOU THINK I HATE YOU
[padrão]
[regra]
Regra padrão-transformação: o índice na transformação indica o termo no padrão.
(0 YOU 0 ME) →(WHAT MAKES YOU THINK I 3 YOU) 

Regras
Regras ligadas a palavras que ocorrem na sentença
Palavras associadas a um ranking
Palavras mais específicas maior o ranking
Resposta com “everybody” (mais específica)
Regra para “Everybody”: (Everybody *) -> (WHO IN PARTICULAR ARE YOU THINKING OF)
“everybody” rank 5 / “I” rank 0 → “everybody” > “I” → escolher padrão para “everybody”
Agent: WHO IN PARTICULAR ARE YOU THINKING OF?
User: I know everybody laughed at me
Regra para sentenças com a palavra “I”: (I *) → (You say you 2)
Agent: YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU

Sem Casamento de Keyword
Não foi detectado padrão
Resposta que não comprometa

PLEASE GO ON
THAT’S VERY INTERESTING
I SEE
…
Uma forma de resolver quando o chatbot não entende algo que o usuário falou

Memória
Armazena conversas passadas e as usa quando nenhuma keyword dá match
Frases com “my”
Quando não entende alguma coisa dita
O sistema armazena conversas para serem respondidas depois
(Atualizar as regras do chatbot)


Implicações Éticas
O risco de as pessoas se envolverem emocionalmente com o programa
Alguns pediram pra ficar a sós com o bot
Usuários não queriam que os diálogos fossem logados -> diálogos bem pessoais

Chatbots
Baseados em Corpus

Chatbots Baseado em Corpus
Usa corpus com conversações ao invés de regras
Corpus: conversas naturais
Ex: ligações telefônicas, diálogo de filmes, tweets, reddit
Criadas usando crowdsourcing
Topical-Chat dataset (11k conversas em 8 tópicos)
EMPATHETICDIALOGUES: 25k conversas focado em emoções
Extrair fatos do Wikipedia ou notícias

Duas Estratégias
By retrieval
Usa recuperação de informação para obter no corpus
By generation
Usa modelo de linguagem ou encoder-decoder para construir a resposta
GPT3 Demo: https://www.youtube.com/watch?v=V-M5SVta2uw 

Limitação
Podem ser repetitivos e monótonos
A: Where are you
going?
B: I’m going to the
restroom.
A: See you later.
B: See you later.
A: See you later.
B: See you later.
A: How old are you?
B: I’m 16.
A: 16?
B: I don’t know what you are
talking about.
A: You don’t know what you
are saying.
B: I don’t know what you are
talking about.
A: You don’t know what you
are saying.

Arquiteturas Híbridas
Baseado em regras, corpus e baseado em frames
http://ai.stanford.edu/blog/chirpy-cardinal/
https://par.nsf.gov/servlets/purl/10318326
Chirpy: construído utilizando as 3 abordagens (regras, corpus, frames)

FORCE_START: regra para iniciar a conversação
STRONG_CONTINUE: requisita um prompt (resposta) que é coletado pelo “Neural Chat”
Movie: identifica a intenção da sentença (falando sobre filmes)
Entidade: detecta a entidade mentionada “The Matrix” “keany Reeves”

Ainda podem ser construídos utilizando:
Wikipedia entity linking (desambiguar entidades) podem ter o mesmo nome e ser coisas diferentes
Classificação de entidade
Classificação de atos do diálogo
Modelos de linguagem para gerar a resposta

Em Resumo
Pros:
Divertido
Bom para aplicações específicas e bem estruturadas (regras)
Limitações:
Não compreendem
Chatbots baseados em regras são custosos pra construir
Basedo em RI: depende muito da qualidade do corpus
Ideal: combinar chatbots com agentes baseados em objetivo
Chatbot para banco: Quais as tarefas o bot vai desempenhar? As mesmas tarefas que no banco físico?

Ideal: objetivo claro e fazer muito bem


Agentes Conversacionais Baseados em Objetivo 

Baseados em Tarefa
Objetivo de resolver uma tarefa para um usuário: reservar um voo ou comprar um produto
Arquitetura GUS
Criada em 1977
Usada pelos assistentes virtuais atuais
Baseada em frames
Frames = formulário

Cada slot é relacionado a uma intenção do usuário


Frame/Template
Conjunto de slots a serem preenchidos na conversação
Cada frame associado a uma pergunta

Modos de Funcionamento
Sistema faz perguntas ao usuário?
Usuário pode preencher vários slots de uma vez?
I want a flight from San Francisco to Denver one way leaving after five p.m. on Tuesday.
Busca na base após preenchimento do frame

Múltiplos Frames
Fonte: https://freshdesk.com/self-service-portal/improve-chatbot-conversation-blog/
Fluxo múltiplos frames para resolução de vários objetivos, fluxo de perguntas, ações a serem executadas de cada vez

Natural Language Understanding
Domain, Intent & Slot Fillers

Arquitetura
Natural Language Understanding
Dialog Manager
Natural Language Generation
Chatbot
Ainda não recebi o meu Xiaomi 12. Quando ele será entregue?
Por favor, digite o número do seu pedido
Natural Language Understanding: identifica intenção, domínio e identifica as entidades/informações recebidas utilizando Modelos de Machine e até Deep Learning

Natural Language Generation: Modelos de Machine e Deep Learning (Modelos de linguagem)

Dialog Manager: é o “cérebro” do bot que contém as regras, gerencia o domínio, a intenção do usuário, e as informações que foram recebidas.. Decide a ação

O Dialog Manager é um elemento mais sofisticado (do que frames) para os sistemas conversacionais baseados em tarefa

É mais complexo que o GUS

Natural Language Understanding
Classificação do domínio
É preciso identificar qual o contexto da conversação do usuário
Trivial para tarefas simples: Alarme, transação sobre um produto, uso de calendário
Necessário em sistemas de diálogo de múltiplos domínios (comuns atualmente)
Determinar intenção: qual a tarefa?
Encontrar um filme, remover um alarme, comprar uma passagem aérea
Slot Fillers: 
Preenchimento dos campos a partir da entrada do usuário
DOMAIN: 	PRODUTO
INTENT:	INFO-ENTREGA
PRODUTO:	XIAOMI 12
Natural Language Understanding
Ainda não recebi o meu Xiaomi 12. Quando ele será entregue?
Essa arquitetura possui componentes para extração de informação lá do texto entregue pelo usuário para completar os formulários
Só que são utilizados modelos de Machine Learning ao invés de regras.

Exemplo
Resolução de entidade: texto “six” -> datetime

Exemplo
Resolução de Entidade: SF -> San Francisco

Preenchimento de Campos
Baseado em regras
Intenção -> Palavras ativadoras

Machine Learning
Classificador para classificação de domínio, intenção e extração dos campos
Necessita de dados rotulados

Dialog Manager
“Cérebro” do sistema
Identificar o estado atual
Salva o contexto
Decide a próxima ação
DOMAIN: 	PRODUTO
INTENT:	INFO-ENTREGA
PRODUTO:	XIAOMI 12
Natural Language Understanding
Dialog Manager
Ação: solicitar número do pedido
Ainda não recebi o meu Xiaomi 12. Quando ele será entregue?
O Dialog Manager vai possuir um rastreador de estados da conversação e uma política de diálogo.

Então ele armazena as últimas coisas que o usuário está solicitando ou conversando com o sistema (contexto)

A política de diálogo decide o que o sistema deve fazer ou dizer

Atos do Diálogo
Descrevem os possíveis estados do diálogo
Os atos do diálogo são utilizados pelo rastreador do diálogo

Cada sistema deve ter rótulos sobre os atos de diálogo para sua finalidade

Os rótulos HELLO(a, b,... ) podem ser vistos como funções

Parâmetros são informações detectadas na conversação para preenchimento de slots

Atos do diálogo
Classificador multiclasse para identificar a tag baseado no contexto
Extrator para identificar os slots
U: Usuário
S: Sistema

Dialogue Policy
Predizer a próxima ação dada toda a conversa

Ou dados campos preenchidos, últimas “rodadas” do usuário e sistema

Baseado em um classificador/busca ou fluxo de conversação
Decide qual ação tomar ou o que responder ao usuário

Confirmação e Rejeição
Lidar com erros do sistema
Ter certeza que o usuário foi compreendido
Dois mecanismos
Confirmação
Rejeição

Confirmação Explícita
O sistema utiliza perguntas diretas para confirmar o entendimento. (Yes/No questions)
É mais fácil para que o usuário corrija o entendimento errado do sistema
É estranho, apresenta um aspecto não natural e não humano na conversação
Prolonga a conversação

Confirmação Implícita
O sistema utiliza a estratégia de grounding: repete o entendimento como parte de uma nova questão
Apresenta um aspecto mais natural à conversação

Rejeição
Uma forma de o sistema mostrar que não entende a entrada do usuário
Ao invés de simplesmente repetir que não entendeu a entrada (Ex: “I’m sorry, I didn’t understand that”)
Prompting progressivo

Natural Language Generation
Gera o texto para o ato do diálogo
Escolhe alguns atributos para colocar no texto
Mostrar ao usuário uma resposta ou pedir confirmação
Preenchimento de template
Em tempo de geração de respostas para o usuário podem ser utilizados templates de sentenças diferentes, com slots que podem ser preenchidos com a informação necessária

A preocupação deve ser em “o que responder” e “como responder”

Encoder-Decoder

NLG usando Classificador
Usa conversações
“Deslexização” de palavras que representam slots
Delexicalizar = mudar a estrutura da sentença

Avaliação
São avaliados por humanos
Avaliação participativa (conversa com o chat)
Avaliação observadora (transcrição da conversação)
Dimensões de qualidade:
Engajamento: o quanto foi satisfatória a interação
Evitar repetições
Fluência
Fazer sentido
Interesse
Curiosidade
Humanidade
Ação de escuta

Avaliação
Pesquisas recentes em avaliação adversarial baseada no teste de turing
Agentes baseados em tarefas não ambíguas:
Slot error rate por sentença
“Make an appointment with Chris at 10:30 in Gates 104”
SLOT
PREENCHIMENTO
pessoa
Chris
horário
11:30
local
Gates 104
Slot Error Rate: 1/3
Adversarial
Gerador: gera o texto
Objetivo: enganar o avaliador

Avaliador: classificador
Objetivo:  avaliar se o texto é parecido com o que um humano escreveria

O sistema de geração deverá enganar o avaliador de forma a melhorar a resposta gerada
Sempre que o avaliador distinguir que o texto foi gerado por máquina, o gerador vai atualizar seu modelo de forma a gerar um texto melhor

Bases de Dados para treino de chatbots
http://yanran.li/dailydialog 
https://github.com/google-research-datasets/ccpe 
https://github.com/google-research-datasets/dstc8-schema-guided-dialogue 
https://github.com/howl-anderson/ATIS_dataset 
https://github.com/amazon-research/nlu-slot-constraints 
https://github.com/MiuLab/SlotGated-SLU/tree/master/data/snips 
https://github.com/MiuLab/SlotGated-SLU/tree/master/data/atis
https://towardsdatascience.com/complete-guide-to-building-a-chatbot-with-spacy-and-deep-learning-d18811465876 

Referências
Dan Jurafsky, James H. Martin. Speech and Language Processing. (3rd ed. Draft). 2021. Disponível em: <https://web.stanford.edu/~jurafsky/slp3/15.pdf>. Capítulo 15 Acesso em: 03 de Março de 2023.

